fast remote memory
FaRM 是一种使用RDMA来改善延迟和带宽的主存分布式计算平台，性能相比于使用TCP/IP主存系统的技术提升了一个数量级。
FaRM把集群中机器的主存作为一个共享地址空间。
FaRM提供两种机制来改善程序性能：在RDMA上的无锁读，支持收集对象和函数迁移来高效使用单机器事务。
它使用RDMA来直接访问共享地址空间的数据和高效的发送数据。
一个20台机器的集群每秒可以执行1.67亿次键值对查询，仅有31微秒的时延。

简介：
        因为把工作集数据全部载入主存中，消除了硬盘或闪存的中间耗费，使得程序可以随机访问小型数据，但是网络通信
    却成为了瓶颈。实验表明：使用TCP/IP的分布式内存计算比单机存储计算性能要差的多，虽然单机环境下数据访问请求代价很高。
    FaRM使用RDMA写来实现快速的消息传递原语，这比在相同以太网环境下比TCP/IP性能提高了一个数量级。使用单边RDMA读对于
    在大多数负载上出现的只读操作实现了两倍的性能改善。但是我们没有非常好的取得这样理想的性能。我们对操作系统和NIC驱动
    仔细的做优化和更改把性能提升了1/8。
        FaRM机器把数据存在主存中，也执行应用线程。这就可以进行局部性优化，这很重要因为访问局部内存比RDMA快23倍。
    线程能够使用ACID的事务而严格串行地在地址空间分配，读，写和释放对象而不需要担心对象的位置。
    事务使用利用RDMA优势带有经过优化的两级提交协议的乐观并发控制。将日志副本写到多个SSD上去，FaRM可以获得可用性
    和持续性，但也可以作为一个缓存。(？)
        FaRM提供两种机制来改善性能，这只需要对代码做一些局部修改：多次无锁读可以使用一次RDMA操作完成，并且这些读是由
    事务保证严格串行的；支持对象收集和函数迁移来允许应用程序把分布式事务替换成经过优化的单机事务。
    我们在FaRM上设计实现了一个新型哈希表算法，它把hopscotch哈希与链式化和关联性结合起来是的空间利用率高，同时只需要
    少量的RDMA读操作来应用于查询操作：在90%的并发上小型对象的读只需要1.04次RDMA读操作。
    通过利用FaRM对收集相关对象和函数迁移的支持，我们对插入，更新和移除操作进行优化。
    
通信原语：
        FaRM使用单边RDMA读来直接访问数据，并且使用RDMA写来实现一种快速消息传递原语。这个原语使用一个环形缓冲区来实现一个单向信道。
    缓冲区放在receiver上，对于每一对sender和receiver只有一个缓冲区。缓冲区未使用的部分（标记为未处理和未使用）是被清零使得receiver
    能都检测到新的消息。receiver间歇性的从Head位置提取数据来检测新消息。头部中任何非零的值L都表示有一个长度为L的新消息。
    receiver然后就轮询提取直到报文尾部：当结果变为非零时，整个消息就被接收完毕，因为RDMA写是以逐渐增加的地址序列来执行的。
    之后，消息被交付给应用层，并且一旦被应用层处理完之后，receiver就将消息缓冲区清零并且将head指针向前(增长的方向)移动。
        发送者使用RDMA来讲消息写到缓冲区的尾部然后每次发送时都将尾指针向增加的方向移动。发送方会维护接收方头指针的一个本地副本并且从来不会
    在写入消息时越过这个点。接收方通过使用RDMA将当前head的值写到发送方的副本里，稍慢一步告诉发送方经过处理的缓冲区空间已经可用了。
    为了减少中间耗费，接收方只在处理完缓冲区的一半后才更新发送方的副本（头指针）。发送方的头指针副本总是滞后于接收方的头指针，因此也就保证了
    发送方绝不会写覆盖未经处理的消息。
        轮询poll的中间耗费随着信道(channel)数量的增加而线性增长。因此我们从一个线程中建立一个连接远程机器的单信道。在78台机器上我们发现轮询poll的
    中间耗费几乎可以忽略不算。我们也发现在这种规模上，RDMA写和轮询poll要比更复杂的infiniband发送和接受动作显著地表现的更好一些。在大型集群上，
    也许使用RDMA带有immediate的写配上共享接收队列SRQ更好，这会使得轮询poll的中间耗费是一个常量。
        我们的实现使用一个连续的环形缓冲区而不是许多缓冲区连接成一个环状来对于大小变化的消息提供更好的内存利用率。此外，【35】里面接收方在消息中
    外带对发送方头指针的更新数据。
        我们在用40G的RoCE网络连接的20台主机的集群环境下跑了一个微测试来比较FaRM的通信原语和TCP/IP的性能。每台机器运行一定数量的线程，以一种全互联
    的通信模式发送从一个随机的远程机器内存中随机读取一块的请求。图2展示了每台机器的平均请求率，每台机器都已经为了达到峰值吞吐量做了配置优化。
    图左边，packet的速率是FaRM通信原语的瓶颈所在，到了右边，通信瓶颈就是比特率。？？？。单边RDMA读操作在2KB的请求大小下达到了33G的比特率，并且
    在请求大小大于8KB时比特率在35G附近饱和。
        FaRM的基于RDMA消息发送模式在16到512字节范围内比TCP/IP的请求速率快11倍到9倍之间，这个数据大小是数据中心应用的典型规模。单边RDMA读操作在
    数据大小提升到256字节时会获得额外的2倍性能提升，因为它只要求一半的网络包（单边的优势）。我们期望这个性能间的间隔能随着下一代支持4倍消息率的NIC
    能持续增加。单边RDMA读操作不干涉远端的CPU而且基于RDMA的发送消息模式将会是CPU BOUND。
        我们也比较了UDP的吞吐量，并且观察到它比TCP吞吐量的一半还要少。
        图3展示了在极限请求速率和为了最小化延迟而只使用两台机器的环境下的平均请求延迟。TCP/IP在极限请求速率的条件下，延迟至少是基于RDMA的发送消息
    模式在所有请求大小下的145倍。对于请求大小提升到256字节时，通过两个额外的因素使用单边RDMA读来减少延迟。在一个没有加载的系统中（负荷不重的情况下），
    RDMA读的延迟至少比TCP/IP低12倍，并且在所以请求大小的情况下比基于RDMA的消息发送模式要低3倍。微测试表明FaRM的通信原语可以同时取得低延迟和高消息率。
        对于这两种通信原语，取得这样的性能水平是不一般的，需要解决一些问题。第一个问题就是：RDMA操作的性能会随着我们增加为远程访问而注册的内存的数量
    显著下降。原因就是：NIC用完了缓存所有页表的空间。因此它不断地通过PCI总线从系统内存中取页表记录。
        我们通过使用更大的页来减少NIC页表中的记录数量解决记录数量过多的问题。不行的是，windows和linux下现存支持的大页不能消除所有的取操作,因为FaRM
    注册的内存太多了。因此，我们实现了phyCo，它是一个内核驱动，可以在系统启动时分配大量物理上连续并且本身对齐的2GB内存区域(2GB是我们NIC支持的最大
    内存页)。PhyCo把这些内存区域映射到FaRM的虚拟地址空间里以2GB边缘对齐。这样我们就能修改NIC驱动来使用2GB的内存页了，而且这把每块内存区域的页表条目
    从超过50万降低到只有一个条目。
        我们运行随机读取测试来比较当内存区域用VirtualAlloc和PhyCo分配时的64字节RDMA读操作的请求速率。图4展示使用VirtualAlloc时，请求速率在NIC注册
    的内存超过16MB时降到原来的1/4。在PyhCo下，即便在注册100GB内存时，请求速率也是保持不变。
        我们也观察到当集群规模增加时，请求速率会显著降低，因为NIC耗尽了存储QP数据的空间。在每对线程之间使用一个QP，每台机器就需要2×m×t×t个QP，m是
    机器的数量，t是每台机器的线程数。我们用单连接来连接一个线程和远程机器把这个QP数量减少到2×m×t。而且，我们以NUMA感知的方式引入在q个线程间共享一个QP
    ，这就使得每台机器一共只需要2×m×t/q个QP对。这用并行化换取NIC上QP数据量的减少。
        我们改变集群规模和q的值来运行64字节传输的随机读取测试。图5展示了q的最优值依赖于集群的规模。q越小，并行化越好，共享耗费越低，也使得在小型集群
    上有更好的性能，但是在大型集群它们也需要更多的QP数据并且引起性能上的损失。我们期望在以后使用Dynamically Connected Transport来解决这个问题，它
    可以通过按需建立连接来改善可扩展性。
        先前的实验表明使用中断和阻塞会将RDMA的时延增加4倍。因此，我们使用基于事件的编程模型。每个FaRM机器运行一个用户级线程，并且把它和硬件线程连接
    起来。这些线程运行一个事件循环来执行应用程序的工作项，轮询poll基于RDMA消息的到来和RDMA请求的完成。这个POLL操作是在用户级上完成，不需要操作系统的干预
    
分布式内存管理：
        FaRM的共享地址空间由许多2GB的共享内存区域组成，它们是内存映射，恢复和NIC用来进行RDMA注册的基本单元。在这个共享地址空间中对象的地址是由两部
    分组成。一部分是32位的区域标识符，另一部分是32位的相对于区域起始地址的偏移量。为了访问一个对象，FaRM使用一致性哈希的一种形式来把区域标识符
    映射到存储这个对象的机器上。如果这个内存区域是存储在本地，FaRM就获取内存区域的起始地址并使用本地内存访问。否则的话，FaRM就和远程机器通信来
    获取对这片内存区域的访问权限然后用它(地址的偏移量和对象的大小)来建立一个RDMA请求。访问远程内存区域的权限会被缓存下来用来改善性能。
        一致性哈希使用one-hop分布式哈希表实现。(???)每台机器通过把他的IP地址和K个哈希函数进行哈希运算映射到K个虚拟环中。FaRM使用多个环来允许多块
    在内存云中对多个内存区域并行恢复，而且也改善了负载均衡。我们目前使用K=100个哈希函数。32位的共享区域标识符能识别一个环和在环上的位置。区域的主备份
    和副本存储在R个机器上紧随着环上内存区域的位置。图7展示了一个简单的例子，k=3和r=3。把内存区域映射到机器上可以在已知集群上机器的集合来进行本地计算。
    集群成员身份可以使用zookeeper来可靠的维持。
        内存分配者是以三级体系来组织的：slabs,blocks,和regions 来减少同步带来的中间消耗(类似于并行分配)。在最低层上，线程有私有的slab分配器，可以
    从大block中分配出小型对象。每个block用来分配同样大小的对象。FaRM支持从64字节到1MB范围内256种不同尺寸。尺寸是可以选择的，因此平均碎片只有1.8%，
    最大碎片只有3.6%。对象是从可以容纳的最小尺寸类别里分配的。Slab分配器用每个对象头部中的一位来标记它已经被分配出去了。当一次分配或者释放对象的任务
    提交时，这个状态被重新生成，在恢复来重构分配器数据结构期间，这个状态也会被扫描。
        blocks是从一个机器范围的block分配器获得的，这个分配器从共享内存中分配blocks。它把内存区域划分成大小是1MB倍数的blocks。每个内存区域有一张
    表，表里包含每个block的8字节的分配状态。这个内存区域是从集群范围的region分配器处获得的。内存区域分配器使用PhyCo为region分配内存然后为NIC注册
    这块内存区域来允许远程访问。它通过随机的选择一个ring和环上的一个位置来为这个region选择一个标识符，这可以确保本地结点存储在主备份上。与region
    和block分配相关的信息在分配时会同样被复制。
        FaRM允许应用程序在分配对象时提供一个分配提示，就是一个存在对象的地址。FaRM尝试以下面的顺序来分配地址：和提示的地址在同一个block里，在同一个
    region里，或者在同一个虚拟ring中和这个region邻近的位置。这确保被分配的对象和提示的地址在主备份和副本上即便失败和重新配置后有很大的概率保持并
    置排列。如果提示的地址是存储在另一个server上，那么这次分配就使用一次RPC对远程server进行执行。
    
事务：
        FaRM支持分布式事务作为一种确保一致性的通用机制。我们的实现是用乐观并发控制和两级提交来确保严格序列化。一个事务上下文记录被事务读取的对象
    的版本号(读集合)，被事务写过的对象版本号(写集合)和它缓存的写。在提交时，机器以coordinator身份来运行事务。向所有参与者发送准备消息而开始事务
    ，参与者是写集合里对象的主备份和复制备份。主备份锁住被修改的对象并且主备份和复制备份在发送回复之前都会记录下这个消息。从所有参与者接收到回复后，
    coordinator就发送validate消息给读集合里对象的主备份来检查被事务读的版本是不是最新的。如果读集合validation成功，coordinator就先给参与者的复制
    备份发送提交消息，再给参与者的主备份发送。主备份更新被修改的对象然后解锁他们，主备份和复制备份都会记录这个提交消息。如果有任何被修改的对象是被
    锁住的，或者如果读集合validation失败了，或者如果coordinator不能收到所有prepare和validate消息，事务就会终止。
        FaRM副本把日志保存在SSD上。为了改善日志性能，他们使用一些MB的非易失性RAM来保存环形消息缓冲和缓存日志条目。当缓冲区填满时，这些条目被清空，
    并且当日志是半满的时候，日志清理也会被触发。这些日志是用来实现一个并行恢复机制，类似于RAMCloud。
        两级提交协议是使用基于RDMA的消息模式实现的，这通过实验展示它的时延很低。这降低了冲突并且通过减少持有锁的时间来改善性能。虽然有这些优化，两级提
    交也许代价比较高而不能在一般情况操作下实现。
        FaRM提供两种机制来在普通情况下取得好性能：单机事务和无锁化只读操作。应用可以通过收集在同一主备份和同一复制备份上执行的事务所访问的对象和通过
    将事务迁移到主备份上来使用单机事务。这种情况下，写集合加锁和读集合validation是本地的。因此，prepare和validation消息是不需要的，并且主备份只需要
    在解锁被修改的对象之前发送一个带有缓存写的提交消息给复制备份。此外，我们使用两种加锁模式：允许无锁读时的模式下，对象先加锁；在独占模式下，主备份
    在更新对象前先加锁对象（在提交消息被提交给复制备份后）。单机事务通过减少消息数量和减小加锁动作的延迟来改善性能。
    
无锁操作：
        FaRM提供无锁读操作，与事务是序列化的并且用不干涉远端CPU的单边RDMA读来执行的。确保应用能在即使对同一个对象有并发写的时候也能观察到一
    个一致性对象状态。FaRM依赖于缓存一致性DMA：它在对象头部的第一个字数据中和每个缓存行的起始处(除了第一行)存储对象的版本号。这些版本对应用是
    不可见的；FaRM自动在读和写上转换对象的内容。
        一个无锁读用RDMA读取对象并且检查是否头部版本号是未加锁的并且是否匹配所有缓存行版本号。如果检查成功，读操作和事务就是严格序列化的。否则
    RDMA就会在随机后退一段时间后重试。图8展示了一个对象的版本域，它占了三个缓存行。
        在事务提交时，对象是通过本地内存访问来写入的。在准备阶段，头部版本用一个比较和交换操作来锁住。我们在头部版本号中使用至少两个标志位来
    编码加锁模式。在提交阶段，对象是这样更新的：先对缓存行版本号写入一个特殊的锁值，然后更新每个缓存行中的数据，最后更新缓存行的版本号和头部版本。
    这些步骤是用内存障分隔开的。在x86处理器上，编译器障足够确保上述要求的顺序。因为DMA在x86处理器上是缓存一致性的，任何RDMA读操作都能通过内存障
    在每一缓存中以强制施加的顺序观察到内存写入操作。因此，对一个对象占的所有缓存行进行匹配版本号可以确保无锁读操作的严格序列化。
       我们使用64位头版本来防止包装但是缓存行版本只保留版本的最低L个有效位以节省空间.我们能这样做是因为执行一次写操作花的时间有一个下限，我们会终止耗时
    超过上限的RDMA读操作来确保一次读操作不会与两次连续的写入相覆盖，两次连续写入会产生相同最低有效位的版本号（这样就无法辨别读取的版本是否正确）。
    这依赖于我们已经要求与zookeeper维持的租赁期的时钟漂移的弱约束。（？？？）。在这篇论文里得到的结果是L=16，但是我们的测量表明在复制备份中
    L=8就足够了。
        为了提供一致性，FaRM必须确保无锁读不会访问那些已经被并发事务所释放的对象。FaRM使用类型稳定性来确保对象的元数据保持有效性，使用前身检查来
    检测对象什么时候被释放掉。FaRM提供128位的胖指针，它包括对象地址，大小和期望的化身实体(incarnation.).应用检查对象缓冲中的incarnation是否匹配
    指针中的incarnation，这个缓冲是通过一次无锁读返回的，这个确保了对象还没有被释放掉。
        FaRM可以重用被释放的内存来分配给其他同样大小的对象，因为对象头部的incarnation仍然有效。(?)对不同对象大小来重用内存需要更多的工作，因为对象
    头部可能会被任意数据覆盖掉。FaRM实现了一个基于时代分配器的分布式版本来做这件事。(?)它给所有机器上的线程发送一个时代结束请求（我们会聚集发给或者
    来自同一个机器的消息）。当一个线程收到这个请求后，它会清空任何缓存的指针，开始新的一轮，继续处理新的一轮中的操作。一旦先前时代一轮中开始的所
    有的事务和只读操作完成了，线程就给请求者发送一个回复。FaRM的API提供bracket匹配操作来使得能够在进行的操作完成时可以检测到的原语。在收到当前配置
    中所有机器的回应之后，内存可以被重用。这个机制对性能没有很大的影响，因为它是在后台运行并且只有在可用内存低于一个门限值时才会生效。
    
哈希表：
        FaRM也提供一个一般的键值存储结构，它是在共享地址空间之上用哈希表实现的。这个接口的重要用途之一就是作为一个超级用户来获取给定key的指向共
    享对象的指针。设计一个使用RDMA运作良好的哈希表类似于其他形式的内存体系感知的数据结构设计：平衡实现好的空间效率和减少需要执行普通操作的RDMA
    的数量与大小很重要。理想情况下，我们喜欢用单边RDMA读来做查询操作，这是最常见的操作。我们把hopscotch式哈希看作是很有希望实现这个目标的方法，因为
    它能保证一个键值对是存储在会被单边RDMA读到的一小块连续内存区域上。这与通常的基于cuckoo式哈希的方法截然不同，在那里面一个键值对是在一些不相连
    区域中的一个里。
        在hopscotch哈希表里，每个篮子(bucket)有一个包含这个篮子和后面H-1个篮子的邻居。Hopscotch式哈希保持这个不变：一个键值对存储在这个键的篮
    子的邻居里。为了插入一个键值对，算法使用线性探测法来寻找一个和这个键的篮子(哈希映射到的)邻近的空篮子。如果这个空篮子是在这个键的篮子的邻居里，
    那么这个键值对就存储在这。否则，算法就会尝试通过重复的移动键值对同时保持不变性条件来把这个空篮子向这个邻居移动。如果这个算法没有找到一个空篮子，
    或者不能维持不变性条件，哈希表就重新调整大小。
        原始算法要比chaining和cuckoo哈希表在高占用率时表现的更好，使用H=32.占用率是被插入的键值对的数量和哈希表中插槽的数量的比例。不幸的是，大型
    邻居用RDMA时表现很差，因为他们会导致大型读取操作。例如，使用H=32，64字节的键值对要求至少2KB大小的RDMA读操作，这比更小的RDMA操作显著表现的更差。
    仅仅使用小型邻居也并不能工作的很好，因为它需要频繁的重新调整大小(哈希表)并会导致空间利用率很低。例如，原始算法在H=8时，平均占用率只有37%。
        我们设计来一个新算法，链式相关的hopscotch式哈希，通过把hopscotch哈希和链式与相关性结合起来，它在空间效率和用来执行查询的RDMA操作的数量
    和大小之间取得很好的平衡。例如：平均情况下，在90的占用率下，H=8时，每次查询只需要1.04次RDMA操作。这比基于cuckoo式哈希在75%的占用率下需要
    3.2次RDMA读操作要好得多。
        新算法每个篮子上使用一个溢出链。如果一次插入没能将空篮子移动到正确的邻居里，它就把键值对添加到这个键的篮子的溢出链上而不是重新调整哈希表。
    这也使我们能限制在插入时的线性探测长度。算法使用结合性来缓解FaRM对象元数据跨越好几个键值对和成链所带来的空间耗费。每个篮子是一个FaRM对象，带有
    H/2个插槽来存储键值对。算法保证一个键值对是存储在键的篮子里或者下一个篮子里。溢出块也存储一些键值对来改善性能和空间效率。
        我们使用FaRM的API来实现这个新算法。哈希表是在集群里的所有机器上共享的。每个机器都分配一些碎片，这些碎片是FaRM篮子的数组，并且和其他机器交换
    指向那些碎片的指针。我们使用一致性哈希来把哈希值在碎片上进行划分来保证弹性。
        查询操作使用无锁只读操作来执行。一次对键K的查询通过发起一次RDMA来读取K的篮子B和下一个篮子B+1。如果在B或者B+1中找到这个键值K，那么这次查询就
    完成了。否则，它就使用无锁读在篮子B的溢出块链表里搜索K。这个链表使用胖指针来把溢出块链接起来，查询操作会检查胖指针中的incarnation numbers是否和
    下一块匹配。如果不匹配，就再次进行查询。把大量的键值对内联式存储在篮子里是不够高效的，因为它会导致大型RDMA操作。FaRM把大型或者大小变化的键值对
    以多个分开的对象来存储，每个篮子存储一个键值(或者大键的哈希值)和一个指向对象的胖指针。如果胖指针中的incarnation number不能匹配对象，查询就会重启。
        版本检查保证每个篮子被读取的时候是独立一致的。然而，对于哈希表的查询，我们也必须确保邻近的两个篮子是彼此一致的。为了做到这点，我们为每个篮子的
    邻接对添加一个joint version，意味着每个对象篮子存储前一个和后一个joint version。如果对应的joint version的值不一样，读取就重新开始。更新邻接
    篮子的事务会增加对应的joint versions。我们使用了我们在对于无锁读操作上减少缓存行版本的大小上使用的技术来降低joint version的空间消耗。这篇论文
    里的结果是使用16位的joint version。
        我们通过将事务迁移到存储相关碎片的机器上来优化插入，更新和移除操作。使用事务可以简化实现，这比查询明显要复杂的多。我们使用FaRM的
    API来确保分片与他们的溢出块并置，因此我们可以使用更高效的单机事务。
        我们向上述那样实现了插入操作。我们使用一个由flat-combining启发的技术来把对相同的键做的并发插入和更新放进单个事务里。这在我们用skewed
    的雅虎云服务测试平台(YCSB)负载的实验里,通过降低对于热点键值的并发控制和复制备份带来的中间耗费，将吞吐量提高了4倍。移除操作尝试collapse
    溢出块链来减少用于查询的RDMA操作的数量。它总是把链中最后一个键值对移动到最新释放的槽位，然后如果最后的溢出块是空的话就释放掉它。否则，
    它就会增加它的incarnation number来确保查询操作可以观察到一个一致性的视图。
        FaRM的哈希表保证线性化并且表现很好。图10展示了在20台机器的集群上，以8字节键值和当键值非规整随机选择时有不同大小的值的尺寸时，
    90%的占用率下，查询操作的吞吐量。它展示了这些结果：当值(values)和不同的邻近尺寸内联时，当值(values)存储在篮子之外时(H=8)。图11展示了
    在同一个实验里的空间利用率：是键值对里字节的总数量与被哈希表使用内存总量之间的比值。结果表明H=8或H=6的内联值对于高达128字节的对象
    上在吞吐量和空间利用率间提供了很好的平衡。可以忍受空间利用率低来获得更好的吞吐量的应用能够把对象内联到320字节，H=2.对象比320字节大的
    就应该存储在表外。
    
相关工作：
        RDMA主要被用来改善消息传递性能，比如一些使用RDMA的MPI实现。FaRM的基于RDMA的消息传递改善了35中的实现。
        一些库和编程语言提供了一个PGAS抽象，在这里面进程既有私有内存又有可以被使用单边操作来远程访问的内存。他们其中有一些使用RDMA来实现单边操作。
    但是和FaRM不一样，他们不支持高效的无锁RDMA读操作。取而代之，他们使用锁，同步障或者消息来确保一致性。除此之外，他们是设计成用于批处理计算并且不
    能很好的适用于建立交互式在线服务，比如：他们缺少对持续性的支持，也没有提供容错或者间歇性的生成检查点。分布式共享内存系统类似于PGAS但是缺乏对
    用户控制的数据并行的支持。FaRM提供一个带有ACID事务的PGAS。
        一些工程已经使用infiniband消息传递原语和RDMA来改善分布式文件系统,HBase和Mem-cached的性能。这些项目使用RDMA来改善特定服务的性能，而FaRM
    提供一个一般性的分布式计算平台。除此之外，他们还使用RDMA来优化消息传递并且不支持单边RDMA读操作，除了7，37.7中的工作支持单边RDMA读操作但是不提供
    一致性保障。
        Pilaf实现了一个使用send/receive verbs来向服务器传输更新操作和单边RDMA读操作来实现查询的 键值对存储。它使用64位CRC来检测不一致读的线性化。
    FaRM检测不一致读操作的技术更加general。它提供对一般事务的序列化。除此之外，FaRM的RDMA感知型哈希表设计表现的更好，因为它要更少的RDMA操作来执行
    查询操作，而且空间利用率更高。直接比较性能很难，因为37中的评估使用在单个服务器上的单核并且没有解决扩展性问题。
        RAMCloud描述了主存键值对存储的日志和恢复技术，但是几乎没有提供对一般情况操作的信息。我们使用对日志和恢复类似的技术但是把它们扩展到解决
    在共享地址空间里对一般数据结构操作的事务。不像39，我们把焦点放在在正常情况下取得好性能的技术。
        和FaRM一样，Sinfonia提供一个带有事务的共享地址空间。它引入了 mini-transactions ，通过把执行放在两级提交协议上，这个能够改善性能。FaRM
    提供优化过的一般分布式事务来利用RDMA配上需要单边RDMA的无锁读操作和使得单机事务可能的局部性优化。
     
结论：
    我们描述了FaRM的设计和实现，一种把应用数据放在主存中并且利用RDMA通信来同时取得高带宽低延迟的新型分布式计算平台。FaRM提供一个共享地址空间和一般化
    分布式事务来简化编程。因为分布式事务对于性能关键操作可能很昂贵，FaRM也提供两种机制来在需要的地方改善性能：无锁只读操作和提供单机事务的局部优化。
    我们通过建立RDMA感知的键值存储和图存储演示了这些技术的有效性。我们的结果表明：FaRM表现很好，它比在相同物理网络上的使用TCP/IP的主存系统在带宽和
    延迟上的性能要好一个数量级。


----------------------------------------------------------------------------------------------------------------------------------------

什么是缓存一致性？DMA缓存一致性有什么用
nic存储页表的容量有限，因此大多将页表存在系统内存中，而把NIC的内存当作一种cache，性能是问题
回调函数有什么鸟用
因为机器映射到K个环上，那么机器就能存储不同的数据，可以并行恢复数据，但是问题是你这不是破坏了数据一致性吗？单个环上的数据存储位置是确定的，但是环与环之间上数据的存储位置就是有差别的？
每个对象在共享地址空间上的地址是：32位region 标识符 和 32位偏移地址，
每个region是2GB，32位region标识符是由  环上的位置和环编号组成，问题是我可以找到数据存储的机器，但是我怎么找到机器上哪个region啊

申请region时，region allocator 会随机选一个环，然后在环上找到离本地结点附近的一个位置作为region identifier，目的是确保主备份在本地结点上。注意，是随机选择一个环，也就是说每个环上包含的内存区域绝对不相交，但是机器上存储的region是可以相交的

复制备份带来的问题，累加消耗内存太大

区域标识符通过一致性哈希映射到环上的机器上
区域标识符由环上的位置和环编号组成。
机器通过一致性哈希映射到环上


事务就是写  读不用事务
会不会有一台机器既是主备份  又是副本备份  ？不可能  因为每块内存区域只会出现在一个环上
给读集合的对象的主备份发送消息是为了检查自己读的对象的版本是最新的
事务机制解决的是在乐观并发控制之后怎么维持内存一致性的问题。
为什么被修改的对象被锁住，事务就会失败	
无锁读就是说读不需要加锁？傻逼吗？结果你还不是需要看别人写操作的脸色，其实就是一看到别人加锁了，你就滚回去呆一会再过来看看能不能读
读写顺序性是通过匹配头部ersion时的memory barrier实现的。


哈希表的实现：
	存储 键和共享对象的胖指针(key,fat pointers)
	使用胖指针是为了应对值的size是可变的
	
	目标是设计一个好的哈希表，每次查询时使用尽可能少的RDMA操作
	就是说查询的条目内容尽可能少，那么一次能查询的条目就尽可能的多，
	进而就能在少数RDMA查询操作中找到目标键对应的值存储在哪(类似于文件系统
	里的inode结点)
	另外还要注意空间效率，就是说利用率要高。
	hopscotch式哈希是指，映射到某个键Key的值应该存储在这个key附近后续H个
	位置范围内，限定范围就能准确估量算法复杂性的上界
	
这个算法得保证键值对存储在一小块连续的内存区域，而且只要一次RDMA读就能找到。
问题是H设大了会导致RDMA读的内容也多，平均下来需要的RDMA操作也多了，
H设小了就可能频繁的重新调整哈希表，导致空间利用率很低。
但是RDMA一次读的内容越多，平均每秒的请求率也就急剧下降，如果能在次数少的
小型RDMA读操作里完成，性能就能大大提高。

仅仅是hopscotch还不够，新算法使用链式结合的hopscotch
改进在于：对于插入操作如果不能在H个邻近的bucket找到空位置，那就把它插入到
这个键的bucket的溢出块链里。好处是限制了线性探测的长度。
每个bucket是一个FaRM对象，带有H/2个插槽来存储键值对。那么这样以来，任意的键必然存储
在当前bucket或者下一个bucket里面。
溢出块链表并不能完全解决resize的问题，只能改善性能和空间效率
使用一致性哈希来把哈希值分布在所有的哈希碎片里确保弹性。
查询搜索是在键Key对应的bucket B和B+1里面找，找不到就去B的溢出块链表里找。
链表使用胖指针来把块链接起来。会把胖指针里的incarnation数和块里面的incarnation数
进行比较，相同就查询完成。
对于哈希表查询，我们必须保证相邻的两个buckets是彼此一致的。加入joint versions
每个bucket存储一个前向joint versions和一个后向versions。
更新邻近的buckets会增加对应的joint versions
