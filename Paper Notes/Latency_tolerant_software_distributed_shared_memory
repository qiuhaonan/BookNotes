
使用RDMA进行底层网络传输

摘要：Grappa，一种针对内存中数据密集型应用的现代式软件分布式共享内存。
      把集群看作是一个单个大型非均匀存储器存取机器。
      即便对于局部性差和依赖于输入的负载分布的应用来说，性能也提升很大
      Grappa通过挖掘应用并行性和用延迟换带宽的方式来解决先前分布式共享内存系统的缺点
      
介绍：数据密集型应用是大规模计算里很重要的一类，这些应用典型的硬件计算设备是通过高带宽的商用网络连接的多核结点的集合，也称为集群。
      软件分布式共享内存提供对集群的共享内存抽象。
      但是由于节点间的带宽限制，结点间的高延迟，使用虚拟内存系统做到全局内存无缝访问的设计，共同使得系统运行效率很差。
      之前的软件分布式共享内存是因为对称多处理器的出现而产生的，但是它只适用于那些表现出很强的局部性，有限制的共享和粗粒度同步的应用。
      对于现代数据密集型应用并不适用。
      Grappa不依赖于局部性来减少内存访问消耗，他依赖于并行化来保持处理器资源繁忙并且隐藏结点间通信的高耗费
      它包含一个将小型消息结合成大型物理网络包的覆盖网络，这最大化商用网络的可用对分带宽      
      通信层是建立在用户空间上，提供全局地址空间抽象。
      支持细粒度的共享而不是页级别的共享带来更好的效率，由于避免了页缺失的消耗，开启在全局内存访问的编译优化
      系统包含三个部分：全局地址空间，轻量级用户任务，聚集通信层
      
      在MAPREDUCE VERTEX-CENTRIC 和RELATIONAL QUERY EXECTION上实现分布式共享内存的难点在于：
      小型消息：共享内存模式下的程序经常访问小数据片导致结点间传输小型消息
               网络上涉及小型消息时，加载和存储操作就成了复杂的事务，相反，如果程序使用MPI，这个复杂性就交给了程序猿，鼓励他们优化它。
      局部性差：数据密集型应用经常表现出很差的局部性，针对图计算时，时间局部性和空间局部性都很差。
      需要细粒度的同步：图并行应用
      
      但是，数据密集型应用大量的数据并行化具有高度的并发性，性能取决于所有任务的总执行时间
      
Grappa设计：
      分布式共享内存：提供系统中任意位置细粒度的数据访问
                    全局内存的每一片由一个特定的core持有，访问远端结点上的数据是通过运行在持有该数据的核上的代理操作完成，读写操作
      任务系统：支持轻量级多线程和全局分布式任务窃取，并发通过用户级线程的合作调度来完成，执行长延迟的线程会自动挂起并自动唤醒。
      通信层：主要目标就是将小消息汇聚成大消息，这个过程对应用编程不可见。接口是基于active message的。
             【active message是一个消息对象，能够自己执行处理过程。是一个用来优化网络通信的轻量级消息协议，重在通过消除与缓存相关的软件代价并使
             应用程序直接从用户级别访问网卡来降低延迟。】
             因为消息汇总与消息拆分必须很高效，所以我们将这个过程并行化而且谨慎的使用无锁同步操作，为了方便，我们使用MPI作为底层消息库和进程的启动
             与销毁。
             
分布式共享内存：
      寻址模式：
            本地内存寻址：局限于grappa系统里一个结点上的一个核所持有的内存区域，访问方式就是传统的指针访问。
            可以访问：与任务相关的栈，从内存的本地核访问全局内存，访问每个系统结点局部的调试基件，本地指针不能访问其他核的内存，只在本地核上有效。
            全局内存寻址：每个核的堆栈上的数据可以被导出到全局地址空间，可以被系统上的其他核访问，使用划分全局地址空间(PGAS)寻址模式，每个地址是job的等级
                        或者全局进程ID和这个进程的地址的元组。
      代理操作：
            代理操作时执行在内存位置本地核的短操作。
            当数据访问模式具有很低的局部性时，在数据所隶属于的本地核上进行修改此数据要比从远端取过来一个数据拷贝然后返回修改过的数据要有效率的多。
            （代理可以普通的实现对全局内存的读写操作，他们也可以实现更复杂的读-修改-写和同步操作）
            代理操作必须在Grappa runtime上被显示的表达.
            它可以执行任何提供的代码而不会引起上下文切换，这保证了所有代理操作的原子性。
            为了避免上下文切换，一次代理操作只能修改被单个核持有的内存区。它总是在它修改数据地址的本地核上执行。
            （不会跨核访问，不会在单核上进行上下文切换，那么...）
            可以确保来自多个请求者对相同地址的代理操作是在单个核上串行执行，带来强独立性和原子性。
            ！！！（问题：如果考虑一个读操作和一个写操作同时发往同一个目的地址，虽然能保证顺序执行，但是会出现读取无效数据的情况，然而这个问题
            不会出现，因为和mapreduce不同的是：mapreduce是隐式对数据操作，用户只要提交任务，就是说谁对哪个数据进行操作是人不可控的，而Grappa是
            将对数据操作的具体逻辑交给编程人员考虑，就是说你既然要读这个数据，你就不能随即修改它，因为这样一来读取到脏数据也是你的错！）
            那么，在竞争性强的数据上的原子操作就会更快，因为天然原子性省去了互斥和锁带来的中间耗费。
            但程序猿想要对分布在多个节点上的数据结构进行操作时，这个访问就必须被显示的表达成多个代理操作和适当的同步机制
            
      内存一致模型：
            通过代理操作访问全局内存允许我们提供一个相似的内存模型
            所有的同步化是通过代理操作完成的，因为代理操作时全局线性化的，他们的更新对系统的所有核都是同样可见。
            某个特定任务在某个时刻只能发出一个同步代理....?
	    （不接受重排顺序）
            这些属性足够保证 内存模型具有顺序一致性，使得程序没有数据竞争。
            
任务系统：
      每个物理核都有一个操作系统线程绑定在上面，所有Grappa的代码都运行在这些线程上
      Grappa上的基本执行单元是task任务，当它准备执行时，它就被映射到用户级的worker线程上，这个线程是在一个操作系统线程内被调度，不同任务之间的
      调度完全在用户模式下完成，不需要操作系统干预。
      
      任务：每个任务是一个闭包，或者是一个函数对象，持有执行代码和初始状态。使用函数指针和显示参数来指定闭包。
           这些对象（大概32字节）持有只读的值和指向公共数据或者同步对象的指针。
           任务对象可以被串行化并在系统范围内传输，最终被一个worker执行
      worker：执行应用任务和系统任务（通信）。它是仅仅是状态位和栈的集合，被分配在特定的核心上。
              任务准备执行时，它被指定给一个worker，这个worker在自己的栈上执行任务闭包。
      调度：执行期间，无论何时只要执行长延时操作，这个worker就会放弃对自己分配的这个核的控制，允许处理核心保持繁忙并且等待操作完成。
            而且，程序猿可以显式的直接调度。
            为了减少上下文切换带来的中间耗费，调度器完全在用户空间执行，只是存储一个worker的状态和加载另一个worker的状态。
            每一个核心都有一个自己的调度器，这个调度器有一个就绪的活跃worker的集合，叫做就绪worker队列。
            每个调度器也有三个等待被分配给某个worker的任务队列。
            前两个队列运行用户任务：所有尚未绑定到核的任务的公有队列，所有已经绑定到他们要修改的数据所属的核上的任务的私有队列。
            第三个是一个优先级队列：它根据确定任务的终止期限制来进行调度，这个队列管理高优先级系统任务，比如间歇性服务通信请求。
      上下文切换：
            worker之间使用非抢占式上下文切换，把上下文切换当作函数调用，只保存和恢复被调用者的状态而不是像抢占式上下文切换里要求的整个寄存器集合
            这需要62字节的存储空间。
            Grappa的调度器设计成支持大量并发活跃的worker，大到他们的上下文数据合起来放不进cache里。
            为了减少对上下文数据不必要的cache miss，调度器显式地管理上下文数据移动到cache中。
            为了实现这点，在调度器里建立一个就绪worker的引用管道，包含 -就绪没有调度，-就绪已经被调度，-就绪且在内存中，三个阶段。
            当上下文预取开启时，调度器只允许运行那些 就绪且在内存中的worker，所有其他worker都被假定为不在缓存中。
            就绪队列经过核实的部分也一定在缓存中。在FIFO调度下，由于空间局部性，队头总在缓存中。
            但一个worker被signaled后，它的引用就被标记为就绪未调度。每次调度器运行时，它的任务就是选一个就绪未调度的worker，将它转换为就绪已调度
            它发起一个软件预取来将任务task朝一级缓存L1移动。

            1000个worker上，上下文切换在50ns级别，500 000个worker上，上下文切换在75ns级别
            不开启预取模式，上下文切换限制于内存访问延迟，大概120ns for 1000 workers。
            开启预取，上下文访问限制于内存带宽
            参考点，同样的实验，在单个核心上使用内核级线程，切换时间对于少量线程是450ns，对于1000到32000个线程，切换时间是800ns
      
      并行化表示：
            为了更好的可编程性，任务从并行循环结构里自动生成，Grappa的并行循环使用递归分解来衍生任务，这会产生一个对数深度的任务树，
            当迭代数量低于用户定义的门限值时就停止执行循环体。
            Grappa循环可以在索引空间或者一片共享内存区域上进行迭代。
            在索引空间上迭代时没有局部性限制，可以被任意的核进行任务窃取。
            在共享内存区域上迭代时，任务被绑定到它在上面执行的那片内存所属的本地核上，如果可能，循环体会优化这个局部性。
            本地的内存区域仍然可以递归分解，以至于如果一个特定循环迭代的任务阻塞，其他的迭代也可以并发的运行在这个核上。
      
通信支持：
      通信层包括两个组件：基于active message的用户级消息接口，支持请求汇集而得到更好的通信带宽的网络级传输层
      
      active message接口：
            在用户层，实现异步active message，它仅仅是C++11的lambda或者闭包
            每个消息由 经模板生成的反序列化指针，闭包的字节到字节的复制和可选的数据有效载荷 组成
      消息汇总：
            通信频繁，那么高效收集和发送消息就很重要，为了实现高效，使用缓存，预取和无锁同步操作
            每个核持有自己的发送消息链表，链表数量和系统的核数量一致，每个结点里的链表允许其他结点互相peek操作
            当要发送消息时，先从内存池里分布一个buffer，决定终点系统结点，将message内容写进buffer，再将buffer链接成对应的发送链表
            这些buffer只被引用了两次，一次是在创建时，很晚后才是另一次：消息被序列化用于传输
            
            在一个结点上每个处理核负责收集和发送最终来自这个结点上所有核的消息到目的结点的集合。
            （谁来做）
            处理核周期性执行一个系统任务：检查发往这个核负责的每一个目的结点的发送消息链表，如果这个链表太长或者已经等了超时的时间，那么所有
            从这个结点到特定的终端系统结点的消息将被复制到网卡可见的buffer来发送。
            实际上消息传输可以仅仅在用户模式下使用MPI来完成，而MPI又使用RDMA
            
            最终的消息组装过程涉及到操作多个共享数据结构（消息链表），所以使用CAS操作来避免高同步开销。
            这个遍历需要谨慎的预取，因为这次大多数发送消息都不在这个处理器缓存中。
            注意到我们使用以核为单位的数组作为消息链表，这个消息链表仅仅周期性的被当前结点里的所有处理器核修改，这在实验中证实比一个全局性以
            每个系统结点为单位的数组作为消息链表要快得多
            一旦远端系统结点收到消息缓存，就会产生一个管理任务来管理解包过程，这个管理任务会在接受系统上每一个核上衍生一个任务来同时解包送给
            这个核的消息。一旦完成解包，这些解包任务就会和管理任务进行同步，当所有的核都已经处理完消息缓存后，管理任务就会给发送方系统结点发送
            一个回复，暗示消息已经成功交付。

为什么不用原生RDMA
            图6右边解释了为什么直接使用RDMA是不够的，数据也显示在infiniband上的MPI的中间耗费可以忽略不计
            集群的网卡不能以线速将小型消息放进网络中：我们集群网卡的RDMA性能峰值时每秒3.2百万次8字节写，而线速极限已经超过了76百万次
            我们认为这种限制主要是由于 发出多PCI总线一次操作的往返时延造成的，而且RDMA网卡对于和CPU同步的支持也非常有限，最后，封装成帧的中间
            耗费也是很大，infiniband的8字节RDMA写操作会发50字节到网线上，而使用ROCE的基于以太网的RDMA则会发98字节，这严重影响有效载荷的比率
            
容错讨论：
      一些最近大数据负载研究表明当下超过百分之九十的分析类任务要求的输入数据都低于1TB且运行不超过1小时。
      Grappa支持这种在中型规模集群上的负载规模，几十到几百个结点，几TB的内存。在这种规模下，像Hadoop系统里的极限容错就是极大的浪费，假设一台机器的
      平均无故障时间是1年，那么我们128个结点集群的平均无故障时间是2.85天。
      我们可以为Grappa添加检查点或者重启功能，使用原生的或者标准的HPC库。
      写一个检查点将耗费分钟级别的时间，比如：我们集群将所有8TB的主存写到它的并行文件系统大概要10分钟，平衡做检查点的耗费和在失败事件中任务丢失后
      从最近的检查点开始带来的耗费很重要，我们可以使用一种先前的方法大概接近最优检查点间隔时间，假设一次检查点的耗时是10分钟，一台机器的平均无故障
      时间是1年，那么我们应该每4.7小时进行一次检查点事件。在这种情况下，重启失败的任务比从检查点恢复任务付出的中间耗费可能更少一些，考虑到这些因素
      我们选择不实现容错机制。
      
相关工作：
      多线程：
      基于硬件实现的多线程延迟容忍包括Decnelcor HEP，Tera MTA，Cray XMT，simultaneous multithreading,MIT Alewife，Cyclops，和GPUS
      硬件多线程通常是单线程性能较低
      作为对主流普适处理器的多线程软件实现，在必要的时候Grappa提供了容许时延带来的好处，使得单线程性能完好无损。
      一个值得注意的结论是：由于顶级存储的时延（L1）和它可以容纳的上下文的数量是直接相关的，容许时延的线程在根本上会受到限制
      然而我们发现预取可以有效的掩盖上下文切换时DRAM的时延，事实上Grappa对轻量级线程的支持和其他用户级线程包的关键不同在于Grappa的上下文预取
      分布式共享内存：
      过去30年里分布式共享内存的许多创新都集中在降低更新的同步成本。
      第一个分布式共享内存系统 包括IVY，使用频繁失效来提供顺序一致性，结果导致对写频繁负载的高通信耗费。
      后来的系统都减缓了一致性模型来降低通信需求，一些系统由于采用多写者协议（延迟对于写到同一页的并发写的完整性）的虚假共享来减缓性能的下降
      通过无锁化和事务访问协议来利用RDMA，Farm提供比TCP/IP 低延迟高带宽的DSM更新，但是远程访问带宽仍然受RDMA的操作速度限制，通常比每个结点的网
      络带宽低一个数量级
      划分全局地址空间：
      一般DSM和PGAS的不同在于远程数据访问是显式的
      分布式数据密集型处理框架：
      HADOOP   Dryad    Naiad    
      
结论：
      工作主要建立在这样的前提下：在共享内存环境下，编写数据密集型应用和框架要比从头开发自定义底层基件简单的多
      为了这个目的，Grappa不是受SMP启发，而是新型超计算机硬件  Cray MTA和XMT机器的启发。
      这次工作借鉴对这些硬件系统的核心洞悉，并把它建成一个调优的软件运行时来从一般商用处理器，内存系统和网络中攫取性能
      
疑问：
      1.分布式共享内存上怎么寻址：a.全局虚拟内存寻址怎么实现，有那些缺点和优点 b.PGAS：怎么实现，优点相比于缺点是否占优势
	  怎么共享：如何保证内存一致有效性，怎么平衡维持的开销和维持带来的效果
	  什么程度上的共享：粗粒度，细粒度

启发：
      当数据迁移通信耗费很大的时候，如hadoop里面需要分发数据，采用“分布式”计算能够减小通信消耗，就是将函数操作送到计算位置上进行计算，可以针对
      数据密集型应用使用并行化计算来解决局部性差的问题
      数据密集型应用大量的数据并行化具有高度的并发性，性能取决于所有任务的总执行时间
