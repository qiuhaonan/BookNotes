摘要：
    可扩展同步化数据结构的实现是出了名的困难。在多核共享内存方面，最近相关工作引入了一种新的同步范式，叫做平面组合，它允许许多并发访问者
    高效的合作来减少在共享锁上的竞争。在这次工作里，我们把这种范式引入另一种领域，在这里面减小通信是最重要的：分布式内存系统。
    我们为容许延迟的PGAS运行时的Grappa实现了一个平面组合框架，并且展示怎么用它来实现同步化全局数据结构。
    我们发现即便使用简单的锁策略，这些平面组合的数据结构扩展到64个结点上也能带来2～100倍的吞吐量提升。
    我们也通过两个简单的图分析来把它转换成应用性能上的提升。Grappa更高的通信开销和结构化并发导致一种新型分布式平面组合，它可以大大地降低
    为了维持全局顺序一致性带来的必要的通信量。
    
介绍：
    划分全局地址空间语言和运行时的目标是提供给程序一个单个共享内存的假象，而实际上是运行在一个分布式内存机器上，比如集群
    这允许程序员编写他们的算法而不需要显示的管理通信。然而，这并没有降低质疑多个并发线程间的一致性的需要。幸运的是，PGAS社区可以利用
    许多致力于解决共享内存问题的工作并且探索不同的开销是怎么导致不同的设计选择的。
    
    【key】
        大多数人都认可，最容易判断的一致性模型就是顺序一致性模型，它强制所有的访问都是以程序顺序来提交，并且以全局可序列化顺序出现。
    为了维持顺序一致性，共享数据结构上的操作应该是线性的，也就是说，这些操作是以全局总顺序原子性的出现。
        
    在物理上共享内存和在划分全局地址空间上，化都呈现出性能问题。最简单的方法是用一个单个全局锁通过简单的互斥独占，来强制确保原子性和线性化
    以这种方法来真正的有序访问通常认为是不可行的，甚至是在物理共享内存上。然而，即便在细粒度无锁同步机制下，随着并发访问者数量的增加，竞争
    也会越来越多，最终使同步操作更加不可行。在多处理结点的集群中大量的并行化和远端同步开销的增长下，问题的严重性就被放大很多。
    
    【key】
        一种叫做横向聚集的新型同步技术控制线程合作而不是竞争。线程把他们的工作交给一个单线程进行代理，这样就有机会把多个请求以特定数据结构的方式
    结合起来，并且执行他们不会出现竞争。这允许甚至一个带有单个全局锁的数据结构比使用细粒度锁或者无锁机制的复杂并发数据结构要扩展的更好。
    
    我们的工作就是将横向聚集应用到PGAS的运行时来减少维持有序一致的数据结构的开销。我们利用Grappa，一个针对细粒度随机访问优化的PGAS运行时库
    它通过高效的在许多轻量级线程间切换来提供容忍长时延的能力。
    我们就正好利用Grappa容许延时的机制来实现许多细粒度同步操作的聚合来实现更高的，可扩展的吞吐量，与此同时还维持线性一致性。
    除此之外，我们还展示了一个一般的横向聚集框架怎么能被用于实现多个全局数据结构。
    然后我们更加深入的解释横向聚集范式和描述它怎么映射到PGAS模型上去。
    接下来，我们解释一些数据结构在我们的框架下是怎么实现的，和他们在简单的吞吐量负载上是怎么运行的。
    
Grappa：
    非常规应用的特点是：有不可预测的与数据无关的访问模式，空间局部性和时间局部性很差。这类应用程序包括：数据挖据，图分析和不同的学习算法，在高性能
    计算方面的关系越来越紧密。这些程序通常发出对不同来源数据的许多细粒度访问，在多核级别上这是一个问题，但在分布式内存机器上就严重的恶化了。
    通常情况是自然的把一个程序映射到一个PGAS系统上会导致大量的通信和很差的访问模式，但是这类应用又不适用一些典型的优化策略比如：数据划分，影子对象
    和批量同步通信变换。幸运的是，这类应用又有一些公共特征：大量的并行数据访问。这种并行化可以在许多不同方式下被挖据来改善整体吞吐量。
    Grappa是一个为商业集群设计的全局视图的PGAS运行时，从头到尾彻彻底底的重新设计来在非常规应用上取得高性能。
    关键就是容许延时。像读取远端内存的长延迟操作可以通过切换到另一个并发线程的执行来容忍。
    鉴于丰富的并发性，我们有很多机会来以牺牲延迟换取吞吐量的提高。
    尤其是这一点：对远端内存随机访问的吞吐量可以通过延缓通信请求并且把它们聚集成更大的数据包来得到改善。
    在共享内存，划分全局地址空间和消息传递范式上的非常规应用高度优化的实现通常都是以同样的结构做的。  
    Grappa把这些收入囊中并作为它的核心基础，并仅仅要求程序猿来表达Grappa可以利用的并发来提供性能。
    以C++11库实现的Grappa编程接口提供对全局共享内存访问和同步的上层操作，也提供任务和表达并发的并行循环结构。
    除此之外，Grappa的标准库也包括管理全局堆，存储像compare-and-swap这样的远程同步操作和一些同步全局数据结构的函数库。
    这些特点使它很适合实现一些下一代PGAS语言，像chapel 和x10。
    下面的章节将会解释Grappa的执行模型和目前的C++编程接口。
    
    任务和worker：
    Grappa使用一个任务并行的编程模型来更容易的表达并发。一个任务就是一个带有一些状态和要执行的函数的函数对象。任务也许会阻塞在远程访问或者同步操作
    上。Grappa运行时有一个轻量级线程系统，它使用预取来在单核上扩展到数千个线程而在上下文切换时间上只有微小的增加。
    运行的时候，worker线程从一个队列中拉取程序猿指定的任务并将它们执行完毕。当一个任务阻塞时，执行这个任务的worker线程就自动挂起直到被某些事件
    再次唤醒，在此之间不消耗任何计算资源。
    聚集通信：
    Grappa中最基本的通信单元是一个active message。为了高效的使用高性能系统中的网络，（这些网络通常仅仅在消息大小是64KB类别上能取得最大带宽）
    Grappa中所有的通信都是通过一个聚集层来发送的，这个聚集层会自动缓存发往同一目的地的消息。
    【key】
        全局内存：
        在划分全局地址空间的模式下，Grappa提供一个散落在集群中节点的物理内存的全局地址空间。每个节点持有内存的一部分，这片内存是分成当前核的worker
        的执行栈，与核相关的本地堆和一片全局堆。
        系统中任何核的所有上述内存都可以通过使用全局地址来寻址，全局地址编码核和核上的地址。除此之外，全局堆的地址是以block-cyclic方式划分的，以至
        于一次大型分配是自动分布在许多结点上。对于非常规应用，这有助于避免出现热点并且对于随机访问通常是足够的。
        Grappa强制严格独立性--即便是在同一片物理内存上的进程间，所有的访问都必须被持有这片内存的核通过一个消息来完成才行。
        然而在编程层次上，这个是隐藏在高层的远程操作背后，在Grappa中叫做代理操作。（意思就是说编程人员根本不知道他要访问的是远程还是本地内存，所以
        统一使用远程操作：发送消息）
        
    图1展示一个代理读操作的例子，它阻塞这个调用任务并且给要读的内存的持有者发送一个消息，然后这个持有者会发送一个带有数据的回复并唤醒这个调用者。
    
    【key】
        横向聚集：
        在最基本的层次上，平面组合的概念就是使得线程间开启合作而不是竞争关系。
        这样带来的好处可以分为为三个方面：
        改善局部性，降低同步和与数据结构相关的优化。
        
    我们会探索这在传统共享内存系统中是怎么生效的，然后描述同样的概念是怎么能被应用到分布式内存中去。
    
    【key】
        物理共享内存：
        仅仅将工作交给另一个核代理，局部性可以得到改善，同步也会减少。考虑图2中的共享同步栈，有预分配的存储和通过锁保护的栈顶指针。没有flat    
        combining的话，无论何时线程想要将一些东西放进栈里，它就必须获得锁，把值放进存储数组里，将栈顶指针向上顶，然后释放这个锁。当多个线程竞争这个
        锁的时候，除了一个以外所有的线程都会竞争失败然后重试，每次尝试获取锁都会带来昂贵的memory fence（串行化加载与存储操作）并且消耗带宽，随着线
        程数量的增加，成功的因子就会骤然下降。相反，在横向聚集下，线程将请求放到公共链表里。他们都尝试获取锁，并且成功的那个线程就成为了combiner。           （既然每个线程获取了锁就能成为combiner，那么就意味着一次只有一个线程能提交请求，而且只可能提交自己的请求，这是废话？）失
        败者不再重试，而是在他们的请求上自旋等待公共记录被填满。（问题是，我是先把请求放进公共记录里再请求锁还是先请求锁再放到公共记录里，如果是前           者，那么怎么保证成为combiner的线程在收集别的请求的时候与这个线程不会发生资源访问冲突，如果是后者的话怎么能实现资源combiner，因为大家都在竞         争锁，东西都还没放进去，我怎么能拿到你们那些没有成为combiner的请求呢，这有点互相矛盾？）这个combiner遍历公共链表，执行所有的请求，完成的时         候就释放这个锁。这使得一个线程在
        cache中保存数据结构，(（可不可能是直接把数据都挂在这里而不是引用？）由于公共链表数组的存储空间是同一分配的，所以操作的存储地址必定在同一页           上，这样就利用了局部性原理，而针对不同数据结构的
        操作是放在一起的，这又利用了局部性原理，因为访问的是相同的地址空间）减小了不同核上线程间的thrashing。它也减少了在锁上的竞争，但是引入了一个
        新的同步点---添加到公共链表。然而，如果一个线程执行多个操作，它可以将它的公共记录留在链表上，分期偿还这个同步消耗。
        （意思就是说我虽然一次同步带来的消耗比较大，但是均摊到我多个执行操作上时得到的消耗就比原来的机制很小了）
        
    公共链表机制可以用其他数据结构来重用，根据需要来保存对方进而实现自己更加灵活的同步。(意思是数据结构可以用以后更高效的数据结构来替换而不会影响
    功能性，只是提高性能)
    上述的代理操作例子是在自行强制发生的。
    
    【key】
        然而，先前工作的关键就是：特定数据结构的优化可以被实现来更加高效的执行这个combined操作。
    
    随着这个combiner遍历公共链表，它把每个非空的公共记录归并成一个合并的操作。在图2中展示的栈的例子中，它遍历链表时（怎么遍历，怎么确定每个线程
    发出请求对内存操作的顺序），线程4仍然跟踪它自己临时栈上的
    操作，当它遇到线程2的pop操作时，它识别出它可以用刚刚从线程3处理的push操作来立刻满足这个pop操作，因此它填满线程3和线程2的公共记录并让他们继续执行
    遍历完剩下的公共链表后，线程向实际的数据结构发出合并后的操作，在这个例子中，两个不匹配的push被添加到栈的顶端。
    在栈这个例子上，合并的是以来自于push和pop匹配的形式，但许多数据结构有其他的操作可以被匹配的方式。
    
    Grappa：
    在PGAS环境中，尤其是在Grappa中，全局同步带来的消耗和并发的数量要比共享内存中的同步和并发量更大。
    每个核心有数千个线程，在一个相当规模的集群上随随便便就会有数百万的worker。平面组合在这有很大的机会能取得利益，就是能大显身手，但是也带来新的挑战。
    图3展示了一个简单的PGAS转换到共享内存栈.(from earlier)
    【key】
        有一个存储数组是在全局堆里分配的，因此它的元素就被切分到系统内各种所有核上去。有一个核实被指定为master来确保全局同步，并持有一个所有并发访问 
        者都认可的数据结构的子项，这个例子里就是栈顶指针。访问这个栈的所有任务都持有这个master对象的全局地址，然后调用自己的代理方法（像之前提到的读 
        代理一样），阻塞在这个任务上直到完成。做一次push操作的样例代码展示在图5上。任务必须给master发送一个消息来获得这个锁。如果成功的话，它就顺着 
        栈顶指针把他的新值写到栈的末尾，在返回来把指针向上移动，最后释放这个锁并返回一个消息来唤醒调用的worker线程。所有其他的worker都阻塞在第一条 
        消息上直到这个锁被释放。Grappa的用户级线程允许请求阻塞而不消耗任何计算资源。然而，每个核上的所有workers线程都执行这个同步并且在单个核上是有 
        序的，这导致核成为了瓶颈。
    
    集中式combining：
    首先想到的也许是直接把flat combining的理念应用到master的串行化上。获得这个锁的worker线程可以继续遍历所有其他等待获得锁的线程的请求然后
    聚合（combining）他们。把这个对应到栈上来的话，这就意味着匹配push和pop（意思是匹配消除），然后只提交剩下无法消除的操作请求，在下一轮combining
    开始之前给所有远程worker发送带有结果的消息（是什么结果？请求已发出的结果还是请求执行完的状态结果？）。这种方法降低了数据结构存储的通信量，但是
    单个核仍然要处理每一个请求，所以如果其他每个核都以相同速度来产生请求的话，这就不能衡量了（请求太多，具体每个核成为了瓶颈）。
    
    【key】
        分布式combining：
        所有的workers都发送独立的同步消息最后把所有的压力都放在一个核上，我们不这样做，而是这样：每个核可以自己先自己的combining然后在批量与master         
        同步把combining分散到每个核上就能使得大量工作都能得到并行执行而不需要通信（大家自己完成自己的combining，把大量通信减小为单次通信即可）在分         
        布式的横向聚集combining里，每个核建立建立自己的公共链表来监测所有等待被提交给全局数据结构的操作。在Grappa里，对于每一个全局数据结构，从与核相关         
        的局部堆中分配一个本地代理对象来拦截请求。从概念上来看，每个worker都把他们的请求提交给本地代理的局部公共列表里，某个核就负责做combining，然         
        后剩下的就是阻塞在原地直到他们发出的请求被satisfied。然而，因为Grappa的调度器是非抢占式的（？），每个worker都能免费的享有原子性直到他们执         
        行了一个阻塞操作比如：通信（也就是等待当前线程自动放弃CPU，因此不会因为外界因素而打断）。这也就是说没有必要去弄一个拥有非常好的同步机制的显         
        式的公共链表。取而代之，worker们直接把他们的操作归并到本地代理里面然后阻塞，除非在一个严格的情况下他们能够立刻满足他们的要求而不会破坏一致性         
        规则。选择的代理结构必须能简洁的收集操作并且高效的执行匹配操作。图4展示了push和pop是怎么在代理的本地栈上做匹配的。在所有局部combining完成           
        后，核上的一个提交请求的worker被选中来进行全局的提交combining后的操作。在图4中，worker4成了combiner并执行和没有combined情况下一样的同步操         
        作，但是呢，它能在一次同步操作里推送多个元素。（问题：在局部combining确实能保证本地发出的请求可以有很好的顺序性和逻辑性，但是在多个结点同时         
        请求改变全局栈上的元素时，他们之间的先后顺序怎么确定呢？或者说这个顺序与全局一致性无关？全局锁在这个里面起到什么作用）仍然要获取全局锁，因此         
        来自不同核的并发combined的请求就必须在master上阻塞和串行化，但是全局同步的粒度很粗（因为是批量提交，批量竞争），这降低了实际的串行度。（就         
        是说批量与批量之间是串行度很好的，但是相比之前一次提交一个请求的情况而言，这种串行度要大得多）
    
    集中式combining可以用在分布式combining上层，就是说在master结点上对提交到master结点上的combined后的操作再进行combining。（不是有锁吗？有锁
    还能有多个同时提交的combined后的操作？）然而，我们实验中试了这个方法，效果提升不大。
    
    【key】
    内存一致性模型：
    在Grappa环境下，顺序一致性保证在一个任务内，操作按任务内规定的顺序，所有的任务都会观察到相同全局顺序。（？是说你看到的和我看到的你的任务内的操作
    顺序是一样的还是说我在全局中的顺序和你看到我在全局中的顺序是一样的？）Grappa的内存模型和C++的内存模型本质上是一样的，对没有数据竞争的（！！！）
    程序保证线性一致性。保守一点，代理操作会阻塞这个发起调用的worker线程直到他们变为全局可见，确保他们可以被用来同步。（？？？）
    如此一来，在一个任务内的代理操作就能确保是以程序的顺序在全局可见并且所有的任务都能观测到相同的全局顺序。对于被同步的数据结构的操作必须提供相同
    的保证。因为这并不是立刻就很明显分布式的水平-combining能保留顺序一致性，我们现在就解释这个原因。
    为了秉承顺序一致性，在特定数据结构上的操作就必须遵循一个全局一致的顺序。提供全局一致性的一个方法就是确保操作线性化，线性化要求在触发API调用期间
    操作是在某个瞬时点上在全局生效。（难道不是吗？）。这确保对于单个数据结构的操作可以有一个一致的总顺序。
    为了操作能够全局线性化，首先，局部combined后的操作的执行必须是可串行化的，这和共享内存式的横向聚集-combining一样，因为合作式的多线程带来的原子性，
    所以这当然是串行化的。第二，combined后的操作必须以全局串行化的顺序原子性的提交。当提交的时候，combined后的操作在特定核上是串行化的，这个核来
    解决同步问题（就是说有一个master core来解决Stack同步）
    只要全局提交一启动，就必须创建一个新的combiner来负责后续的操作。如果要使用原来的combiner的状态来局部的满足请求，那么就会破坏全局的顺序因为
    局部对象不能反映其他核的更新操作。（意思是每次combiner使用完毕后，它的状态对于下一次是没有参考价值的，因为全局数据结构上的更改不仅仅是本地核
    进行提交过，还有其他核提交的操作，也就是说全局数据结构不可由本地所作的操作来预测，具有不可知性）
    要使这一系列串行批处理操作进行一系列级联的生效，那么worker在局部combining期间得到的操作顺序必须和从全局得到的被提交的操作的顺序相同。
    在栈和队列的情况下，这种保证来源于在全局数据结构的一个连续块上原子性的提交一批push或pop操作。
    为栈数据结构在局部进行匹配push和pop操作是操作必须阻塞直到被全局提交这个规则中的一个例外。因为一个pop操作破坏（抵消）对应的push操作，这些操作
    都是与其他所有操作相独立的（独立是什么意思？），并且可以从概念上以任意的全局顺序被放置到任意位置（因为这是一对互补操作，对真实数据结构没有影响
    但是，问题是在push和pop时在逻辑上不能有其他操作再对同一个数据结构进行操作，换句话说，它又把逻辑复杂度抛给了上层编程），没有必要以程序中的顺序
    来安排它们的顺序，因此可以局部安全的匹配。
    combining集合(set)和映射(map)操作则在一致性需求上出现更多的细微差异(？)。由不同任务执行的插入和查询操作本质上是无序的，除非在外部进行同步化      
    （？）因此，一批这样的操作可以并行的向全局提交，因为他们一定不会彼此之间互相干扰(？)。注意到如果一个插入操作发现它的键已经在combiner对象里时，它 
    就没有必要发送它自己的消息了。但是，它仍然需要阻塞直到插入操作完成，来确保它接下来执行的操作不能被它重新排序，保护程序的顺序。（意思是 大家的操作
    是同步的，你不能因为你要做的已经被别人提交了而提前做你接下来的操作？）同样的是，查询可以借鉴（搭便车）其它对同样键值的查询结果。现代处理器使用来 
    自存储或者写的直观性(intuition)，尝试允许一次查询操作可以通过检查尚未完成的插入操作而就近得到完成。然而，这将允许插入键的局部顺序被观测到。然后
    为了维持顺序一致性，我们也要从全局考虑同样的顺序，迫使每次原子性批量提交时要考虑其他核的批量提交。但强制这样就会寸步难行，因此一种比较聪明的做法 
    就是：查询操作仅仅从全局数据结构那获取他们想要的值，因此他们的批操作可以并行处理（？？？难道奥把查询操作与其他操作分隔开来？）！这些要求仅仅保证
    对单个数据结构操作的线性。对于所有访问来说，为了保证顺序一致性，则必须由程序来确保没有数据竞争，这和C++里共享内存的内存模型一致
    
Grappa的FC框架（flat-combining）：
    FC框架解决了常见的问题：管理combiners，解决worker的唤醒，维护进程。
    当连接到框架内部时，每个数据结构只需要定义怎么合并操作和全局提交他们。
    和先前提到的一样，Grappa的FC框架采取了一种不同的解决方法而不是原本的flat-combining做法来表达怎么合并操作。对于每一个全局结构实例，
    每个核上会创建一个代理对象并且每个worker在阻塞前或者成为combiner之前将他的请求归并到这个结构中。每个全局数据结构必须定义一个代理，它有跟踪合并
    后的请求的状态(state)，有添加新的请求的函数(method)和一个全局性地同步状态的方式。
    FC框架负责确保所有合并后的操作最终得到提交。有很多方法可以确保这个过程，但是最简单的就是保证：只要有任何没有完成的请求，至少有一个worker在提交
    一个合并后的操作。当合并后的操作完成后，如果在这期间产生了仍然没有完成的请求，那么另一个阻塞的worker线程就被选中来做另一个combined同步。
    当一个合并操作在执行时，新来的请求就持续累积。这个框架透明地wrap（内含）代理对象因此当一次同步开始时，它可以把请求送给新的combiner。
    这是通过将代理对象的实例隐藏在C++类似智能指针的对象里面来完成的，这个对象提供指向当前combiner的指针，无论何时它检测到它应该发送请求时，就分配
    一个新的combiner并且把随后的应用指向这个combiner。
    
    
相关工作：
    虽然很多注意力都集中在无锁化数据结构上，比如：Treiber Stack，Michael-Scott Queue，一个值得注意的工作已经探索了使用combining全局地缩减
    加锁的数据结构来减少同步和热点。技巧主要是在用来做combining的结构上有区别：Fixed tree,dynamic trees（funnel 漏斗），和随机树。
    特别是为MTA-2做的MAMA利用funnels来更好的利用针对大量并发做过优化的硬件。在另一方面，flat-combining观察到在多核系统里 层次化的方案有太多
    中间耗费，一个实现精巧的flat 公共链表可以表现的更好。下面的工作引入了并行flat-combining，在这里多个公共链表并行的combine然后剩下的操作
    被串行化。
    
    
数据竞争：
    1.同一个进程中多个线程访问同一片内存区域
    2.至少有一个线程执行了写操作
    3.没有使用独占锁来控制他们的并发访问
    
Data Race Free 的动机

Data Race Free 是对多线程程序 同步程度 的一种描述，假如你的多线程程序的同步程度满足 DRF 的要求，那么，你的程序会有这样一个好处：

程序在弱一致性模型下执行，执行的结果与在SC模型下执行一样
这意味着，程序员在写程序时，可以按SC模型来推断程序的执行。而程序在底层运行时，可以享受弱一致性模型带来的种种优化措施。

Data Race Free 具体内容

DRF 要求多线程程序中不能有冲突的操作。

什么是冲突的操作呢？

冲突的操作是指：两个操作来自不同线程，操作同一地址，至少有一个是写操作。

如何让冲突的操作不冲突呢？

需要使用同步操作将冲突的操作隔离开。

为什么要用同步操作将冲突的操作隔离开呢?

因为如果不隔离开，程序在弱一致性模型下执行的结果就和在SC模型下执行的结果不一样了。这意味着如果你用SC模型推断程序执行结果，
而程序又运行在弱一致性模型下，那么程序的真正结果可能和你推断的不一样。

那么，又为什么：如果不隔离开，程序在弱一致性模型下执行结果就和SC模型下不一样了呢？

这个问题其实在问：为什么隔离会使得程序在弱一致性模型下执行结果与SC模型下执行结果一致？

这个问题用一句话来回答是：隔离使得我们可以找到所有操作的一种全序，而这种全序正是SC所需要的。

同步操作将相互冲突的操作隔离开，这种隔离为原本无序的多线程程序添加了一些顺序：

同步操作之间有顺序了
同步操作与其之前的所有操作之间有顺序了
同步操作与其之后的所有操作之间有顺序了
这些顺序保证了程序在弱一致性模型下与在SC模型下执行结果一样。

另外，我们还发现，有些操作之间并没有顺序保证，这正是DRF的优势所在，这些无须顺序保证的操作可以在弱一致性模型下得到优化，
同时他们的无序又不会使得执行结果与SC下有任何不同。

如果我们想找一个所有操作之间的全序，只需要在这些无须保证顺序的操作中随便选择一个顺序，另外，还需要保证那些因为同步而添加的顺序关系。
如此构成一个全序。这个全序正是SC模型所需要的。

由此，我们也明白了DRF的精髓:

只保证必要的顺序，不保证不必要的顺序。
所谓必要是指，保证这些顺序就可以使得程序在弱一致性模型下的执行结果与SC模型下的执行结果一致，不保证就不行。
