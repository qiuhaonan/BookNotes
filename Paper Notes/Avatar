1.背景
由于RDMA技术自身的内核旁通，远端结点旁通特性和网卡支持的极低延迟网络特性，可以给分布式应用带来极大的网络性能提升，也因此受到数据中心应用
的青睐。现在许多国内外公司都已经在自己的数据中心中大规模部署RoCE网络和RDMA应用。
现在的RDMA网卡支持的网络资源数量上限很高但底层发挥关键性能的硬件资源有限，网卡承载大量网络资源时，有限的硬件资源成为网络性能的瓶颈
而数据中心的分布式网络应用数量繁多且对网络资源消耗严重，随着部署分布式应用的结点数量规模增加，上层应用持有的网络资源总量远大于网卡硬件资源
可以容纳的范围，造成大量随机服务资源未命中现象，随之带来的是耗时的资源搜索和加载过程，降低网卡服务性能，进而影响上层应用的服务性能。
为了适应大规模数据中心适配高性能RDMA网络的需求，我们不得不解决应用持有的网络资源数量和网卡自身的硬件资源数量之间的巨大鸿沟。
而解决这一鸿沟的主要思路有两个，一个是增加网卡硬件资源数量，另一个是减少上层应用持有的网络资源数量
第一个思路产生的做法很直接但是成本很昂贵
第二个思路带来的想法是在上层应用之间对这些网络资源的复用和共享，减少对网络资源的消耗以维持网卡服务性能

2.相关工作
人们开始用各种方式来探索对资源共享和复用的方式：
NSDI‘2014 微软研究院针对网络连接QP提出最初的多线程互斥共享模型，在不同规模的集群下给出了相应的参考共享粒度以最大化网络服务性能，这种做法指示了我们减少
上层应用持有的网络资源数量可以提升网络性能是可行的，同时也从侧面反应这种多线程互斥共享方式在不同的集群规模下产生的锁竞争带来了性能损失，因此
需要一种新的能够减少网络资源的同时避免不同程度锁竞争带来的性能损失
ODSI‘2016 CMU的Anuj Kalia在针对小数据的分布式事务场景下巧妙的利用了RDMA不可靠传输类型的特点，使用UD来实现一对多通信，极大的降低了网络资源消耗，另外
它使用协程来代替多线程，避开了线程对争夺QP锁的问题，在应用性能上取得了不错的结果，这种做法再一次验证了减少上层应用持有的网络资源数量来提升网络性能是有效的
SOSP’2017 Purdue的Shin-Yeh Tsai在这一问题上也是沿着第二个思路来看待问题，和FaRM以及FaSST不同的是FaRM他们的入手点是在现有的网络资源管理体系中针对特定问题
提出有针对性的解决方案，而Shin-Yeh Tsai则退后一步，重新看待整个体系，发现整个网络资源管理框架不够合理，网卡在收发数据的同时还在承担着地址转换，访问安全检查和全局
网络连接管理的任务，而这些多余的任务正是影响网卡性能的重要因素，基于这个分析，他认为网卡不应该承担巨大的资源管理压力，相反这些管理任务应该向上移交给内核软件层，
网卡只负责收发数据，因此对于地址转换和访问安全检查，他利用注册物理内存的方式从网卡驱动层移除掉，在内核层插入一层主动的地址转换操作和访问安全检查机制，而对于网络连接
管理方面，他仍旧利用互斥共享的方式来实现资源复用，虽然存在锁的问题，但是网卡由于前两项任务的释放，性能的提升在一定成都上弥补了这一性能损失，因此，在这个问题上他
更注重于复用资源之后如何保证上层各个应用的性能隔离和QoS，他在实验结果中观察到仅仅靠网卡硬件提供的QoS方式在灵活性和性能比不上软件手段实现的QoS,但本文并没有给出
一个具体的方案来保证应用的QoS.

因此，从以上这些工作来看，
我们要做的是两件事情，一个是解决一个没有被很好解决的问题：到底该怎么共享网络连接才不至于引入额外的性能损失；另一个问题就是对于上层应用，如何保证数据流的QoS服务

3.我们的工作
问题1：到底该怎么共享网络连接才不至于引入额外的性能损失
    分析：略
    解决方案：略
    结果：略
    与其他方案比较：略
问题2：对于上层应用，如何保证数据流的QoS服务
    
    分析：
        LITE QoS实验设置：
            20个低优先级任务(各自发起600K个请求)：5个线程做4KB write操作，5个线程做8KB write操作，5个线程做4KB read操作，5个线程做4KB read操作
            20个高优先级任务(各自发起200K个请求)：10个线程做4KB write操作，10个线程做4KB read操作
            优先级QP设置：3个高优先级QP，1个低优先级QP

        Shin-Yeh Tsai做的实验中得出网卡硬件QoS的灵活性和性能不如软件手段实现的QoS方式，这是为什么呢？
        这里所说的性能与灵活性相关，如果不够灵活，性能就会出问题；从Shin-Yeh Tsai的实验结果中看到靠设置硬件高优先级队列来向上
        提供QoS服务的带宽仅仅占所有流量总带宽的一半左右，和不设置硬件优先级队列的情况下测出同样的流的带宽相差无几，也是占所有流量带宽的
        一半左右，但这是不是意味着网卡硬件QoS无效呢？是不是上层应用的行为与网卡硬件QoS行为不匹配导致的效果不好呢？Shin-Yeh Tsai文章
        中没有给出相应的解释和探索
        为了更好的做应用的QoS,我们还需要进一步对网卡硬件优先级队列的调度行为做深入了解
        从下向上分别涉及到：网卡硬件优先级队列的调度性能测试，多条流到同一个网卡硬件队列的排队测试
    
    网卡硬件QoS分析：
        网卡目前的QoS策略：
            调度的基本单位是优先级队列，也就是在网卡上的硬件优先级队列之间做调度
            优先级队列中处理的基本单位是硬件格式的WQE，WQE索引数据在内存中的位置和长度
        
        调度性能测试
        实验1：观察网卡在启用和关闭硬件优先级队列两种情况下，高优先级流的服务情况
            主实验：
                设置一个strict严格优先级队列和一个ETS优先级队列
                安排一个16KB的背景流进入ETS队列，一个16KB的测试流进入strict队列(WQE Batch均为10)
                两个流同时启动，但是背景流是infinitely模式，而测试流是执行8K次(每次10个WQE)任务后停下来，记录下每次任务的完成时间
                为了排除其他因素的干扰，更清楚的观察网卡硬件QoS的能力，所有流的应用I/O路径缩短到仅仅是不停的发送数据到网卡队列上

            对照实验1：不做优先级归队，将主实验中的两个流映射到同一个网卡队列中，其他不变
            对照实验2：将主实验的测试流和背景流大小改为64KB，其他不变
            对照实验3：将主实验的测试流和背景流大小改为128KB，其他不变
            对照实验4：对实验2不做优先级归队，其他不变
            对照实验5：对实验3不做优先级归队，其他不变

            对照实验6：将主实验的测试流任务改为单次批量任务(一次性发起8000个WQE)，统计总任务的完成时间
            对照实验7：对照实验6不做优先级归队，其他不变
            对照实验8：将对照实验6的测试流和背景流大小同时改为64KB，其他不变
            对照实验9：对照试验8不做优先级归队，其他不变
            对照实验10：将对照实验6的测试流和背景流大小同时改为128KB，其他不变
            对照实验11：对照试验10不做优先级归队，其他不变


            对比主实验和对照实验1，对比对照实验2和对照实验3
            结论1：流小的条件下，对于在时间上连续性低(发起很多次，每次batch量小)的流来说，启用优先级队列的效果和关闭优先级队列几乎相同
            结论2：流小的条件下，对于在时间上非常连续(发起少数几次，每次batch量大)的流来说，启用优先级队列的效果没有明显改善
            结论3：随着流大小
            的增加，无论流的模式如何，启用优先级队列有明显效果
            结论4：优先级队列的调度效果取决于调度粒度，而调度粒度就是WQE的大小

        调度性能测试
        实验2：观察网卡在启动硬件优先级队列情况下，高优先级流受到低优先级流大小的影响程度
            主实验：
                设置一个strict严格优先级队列和一个ETS优先级队列
                安排一个16KB的背景流进入ETS队列，一个16KB的测试流进入strict队列(WQE Batch均为10)
                两个流同时启动，但是背景流是infinitely模式，而测试流是执行8K次任务后停下来，记录下每次任务的完成时间
                为了排除其他因素的干扰，更清楚的观察网卡硬件QoS的能力，背景流的应用I/O路径缩短到仅仅是不停的发送数据到网卡队列上

            对照实验1：将背景流大小改为64KB，其他保持不变
            对照实验2：将背景流大小改为4KB，其他保持不变
            对照实验3：将主实验的测试流任务改为单次批量任务(一次性发起8000个WQE)，统计总任务的完成时间
            对照实验4：将对照实验1的测试流任务改为单次批量任务(一次性发起8000个WQE)，统计总任务的完成时间
            对照实验5：将对照实验2的测试流任务改为单次批量任务(一次性发起8000个WQE)，统计总任务的完成时间

            对比主实验和实验1,2：低优先级背景流的大小越小，高优先级测试流受到的影响越小，总完成时间越少
            对比实验3,4,5：高优先级流的完成时间不受背景流大小的影响，启用优先级队列效果接近无背景流的效果

            结论：启用优先级队列的效果(实时性)受到背景流的大小影响，也就是受到网卡处理的粒度(WQE)的影响
                 限制低优先级队列的流大小有助于提高高优先级队列的流服务质量，提高调度的实时性
                 离散任务效果改善明显
            

        同一优先级队列的排队性能测试
        实验3：观察多个流映射到同一个网卡队列上时，其他流大小和流长度(batch_size)对某个流的影响
            
            1.流大小主实验：  
                启动两个流，一个16KB的背景流，一个16KB的测试流(WQE Batch均为10)
            2.流大小对照实验：  
                启动两个流，一个64KB的背景流，一个16KB的测试流(WQE Batch均为10)
            结论：16KB流的时延受到64KB流严重影响

            3.流长度主实验：
                启动两个流，一个16KB的背景流，WQE Batch为50，一个16KB的测试流，WQE Batch为10
            4.流长度对照实验：
                启动两个流，一个16KB的背景流，WQE Batch为100，一个16KB的测试流，WQE Batch为10
            结论：背景流的WQE Batch越大，测试流时延受到的影响也越大

            启示：享受同一优先级服务的多个不同大小和长度的流之间需要一种公平排队机制

        解决方案：
            针对高低优先级服务的策略：
                限制低优先级队列的WQE大小，提高网卡对优先级队列调度的实时性
            针对统一优先级的各个不同大小流的服务策略：
                使用基于虚拟时钟的排队算法来代替网卡对各个CPU TX的RR或者FIFO做法
        结果：略
        对比Shin-Yeh Tsai的做法：
            在高低优先级任务的QoS层面:我们的解决方案更多的是挖掘网卡硬件QoS特性来增强QoS效果，不需要软件手段去检测高低优先级任务的负载来做速率限制
            在QoS的深度层面：我们向下再做一层，关注到每个流的时延公平性服务
        为什么一定要在主机端做好同一优先级内多个流的公平服务？
        因为网络端(交换机，路由器)只会对不同优先级做到QoS服务保证，不会对同一优先级的流有所区别，除非在应对网络拥塞时会利用ECN标记的算法来限制到每个流
        的速度，因此网络端只会重复转发从主机端收到的网络数据，模拟主机端的行为，最终数据到达另一端。可以看出主机端在对同一优先级多条流的调度具有决定性意义。


1.Abstract
2.Introduction
3.Design Consideration
    3.1 同步互斥共享资源
    3.2 RoCE QoS体系机制和网卡硬件QoS特点
        架构和机制
        网卡硬件队列调度特点
        网卡硬件队列排队特点
4.Design of Avatar
    4.1 异步无锁共享资源机制
    4.2 平台QoS机制
5.Eval
6.Related Work
7.Conclusion

故事：
    从RDMA可扩展性问题入手，强调资源的复用对网卡性能的重要性，因此我们必须对资源进行复用。
    指出目前复用资源的方法存在缺陷，提出一种新的复用资源方式可以弥补这些缺陷。
    复用资源后得对上层复用的应用QoS保证，而现有的QoS做法粗糙并不完善，需要一种新的QoS保证
    如果不考虑网卡硬件的QoS特点，那么上层应用基于优先级的QoS策略就不能获得保证的服务(LITE中提到该现象)
    因此我们需要对网卡硬件的QoS特点进行探索
    发现网卡硬件QoS存在缺陷，但是换个角度来看，问题就是上层应用流的行为和网卡QoS的模式不匹配
    因此可以通过上层应用流的策略匹配网卡硬件QoS的特点
    从而实现最大化QoS效益

我现在要做的事情是：
不去follow别人的工作，从自己做研究的角度出发

所以我的研究故事是：
    现在由于RDMA的高速网络数据传输特性，在数据中心中应用广泛，在小型集群中,RDMA性能表现优秀，但随着集群规模的增加
    这种性能优势变逐渐在消退，原因是RDMA网卡性能受到网卡片上内存大小的影响，随着网络连接数量和注册内存区域的增长，
    RDMA网卡性能逐渐下降。因此我们需要解决网络资源数量大和片上内存小之间的不匹配问题。有效的办法是：增加网卡片上
    内存；减少网络资源使用量。增加网卡片上内存的成本昂贵，可行性低，因此需要对资源进行复用来有效减少大规模集群下消耗
    的网络资源总量，以维持网卡性能,直接对资源进行同步复用会带来对资源的竞争，可扩展性存在缺陷，需要设计一种新的资源复用
    方式来改善资源复用性能。而资源复用后带来的一个问题是对共享资源的应用提供相应的QoS保证，现有的硬件QoS策略
    是基于流优先级的流调度，但现有硬件QoS策略不足以满足复用资源的应用性能需求，需要设计一种新的QoS策略来更好的利用现有
    硬件QoS策略，保证复用资源的应用得到有效的QoS服务。
    因此我们工作的贡献点是两部分：提出了一种新的异步资源复用方式，解决同步资源复用的可扩展性缺陷；设计了一种有效的应用QoS
    机制，改善了硬件QoS机制的缺陷  

我们设计的QoS策略：
    目标：
        1.使得同样大小(16KB,64KB,128KB)的高低优先级的流同时存在时，高优先级的流在时延方面能够比不做QoS下的时延有改进
          凸显优先级调度带来的效果
        2.使得不同大小(16KB和64KB,128KB)的同样优先级的流同时存在时，小流在时延方面能够比不做切分QoS下的时延有所改善
          凸显切分带来的效果
    设计：
        1.在准备数据阶段对流本身进行切分，高优先级的流切分大一点(4KB)，低优先级的流切分小一点(4KB)，高低优先级的流进入到不同优先级的QP中
        2.在发送数据阶段对流整体进行调度，根据流的优先级权重，先发高权重流的数据，后发低权重流的数据
    实验：
        1.启动一个高优先级 16KB的流，执行8000次任务，每次post 10个WR到队列中，记录下时间并输出
          启动一个低优先级 16KB的流，无限制发数据直到程序结束，每次post 10个WR到队列中
        2.将实验1的流大小都改为64KB
        3.将实验1的流大小都改为128KB
        
        4.启动一个 16KB的流， 执行 8000次任务，每次post 10个WR到队列中，记录下时间并输出
          启动一个 64KB的流， 无限制发数据直到程序结束，每次post 10个WR到队列中
        5.将实验4的背景流大小改为128KB
        
        6.将实验4的背景流batch size改为 50个
        7.将实验4的背景流batch size改为 100个

        8.将实验5的背景流batch size改为 50个
        9.将实验5的背景流batch size改为 100个

为了实现这些实验要求，我需要对程序做什么改动？
1.首先修改切分内存块大小
2.需要能够对每个虚拟连接请求的完成予以时间标记，需要在Connection内部设置一个专属的start_time和end_time，然后通过一个is_complete方法
来检查之前发起的请求是否被完成，以及中间所需要耗费的时间，为了能够让poller能够识别本地send_cqe是由哪个虚拟连接产生，需要在wr_id上设置connection的64位值
然后转化成connection，最后打上end_time时间戳。因此暂时取消原本wr_id的page值，也不对page进行打使用标记
3.关闭qp的sig_all选项，每次在send中对最后一个wr进行设置IBV_SEND_SIGNALED,其他都设置为0
4.当发送数据过大时，切分内存块，入队的时间过长
5.底层设置两个QP，第一个设为低优先级，映射到队列1，优先级0
                第二个设为高优先级，映射到队列0，优先级1
  当虚拟连接的sl大于5的时候，数据进入到队列0中，否则进入到队列1中

send_cqe没有imm_data，因此不能针对连接进行计数

指明问题
强调细节和性能保证
