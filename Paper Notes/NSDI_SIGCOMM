NSDI'18

1.Balancing on the Edge:Transport Affinity without Network State
内容交付网络和边缘对等设施具有独特的操作约束，这要求新颖的负载平衡方法。与传统的集中式数据中心网络相反，物理空间受到严重限制。这种限制既能提高对效率的需求，又能最大限度地吸收拒绝服务攻击和边缘闪光拥挤，并实现无缝故障切换，从而最大限度地减少维护对服务可用性的影响。
本文介绍了Faild，一种分布式负载均衡器，它运行在商用硬件上，可以在不依赖网络状态的情况下实现正常的故障转移，为现有提议提供了经济高效的可扩展替代方案。 Faild允许边缘网络的任何单个组件在不破坏现有连接的情况下从服务中移除，这种特性在过去四年中已被证明有助于维持大型全球边缘网络的发展。作为这一运营经验的结果，我们进一步记录了由于配置错误的设备导致的意外协议交互，这些交互设备对传输协议设计有重大影响。

2.Copa:Practical Delay-Based Congestion Control for the Internet
本文介绍了Copa，一种端到端的拥塞控制算法，它使用了三种思路。首先，它表明目标速率等于1 /（δdq），其中dq是（测量的）排队延迟，在马尔可夫包到达模型下优化吞吐量和延迟的自然函数。其次，它调整其拥堵窗口的目标速率的方向，甚至在面对重大的流损耗情况下，能迅速收敛到正确的公平速率。这两个想法使得一组Copa流在低排队延迟的情况下保持高利用率。但是，当出现基于丢失的拥塞控制流共享缓冲区时的瓶颈时，Copa与其他延迟敏感方案一样，实现低吞吐量。为了解决这个问题，Copa使用了第三个想法：通过观察延迟演化来检测缓冲填充物的存在并且对δ参数进行加法增加/乘法减少作出响应。实验结果表明，与BBR和PCC不同，Copa优于Cubic（相似的吞吐量，低得多的延迟，对于不同的RTT更公平），BBR和PCC（更公平，更低的延迟），并且与Cubic共存。 Copa对非拥塞损失和大瓶颈缓冲区也很有效，并且在长RTT路径上优于其他方案

3.Larry:Practical Network Reconfigurability in the Data Center
现代数据中心（DC）应用需要高交叉机架网络带宽和超低，可预测的端到端延迟。在传统DC网络中，很难满足这些要求，其中架顶式（ToR）交换机与DC其余部分之间的带宽通常会超额订购。 Larry是一种网络设计，它允许机架根据流量需求动态调整带宽到汇聚交换机的带宽。 Larry重新配置网络拓扑以使需求高的机架能够使用来自其邻居的未充分利用的上行链路。工作在物理层，可预见性地知道Larry的流量转发开销很低，适用于对延迟敏感的应用。即使部署在少量机架上（例如4台机架），Larry也很有效，因为机架流量需求在许多DC工作负载中并不相关。它可以逐步部署，并与现有的不可重新配置的机架共存。我们的原型使用我们建造的40 Gbps电路开关，并带有一个简单的本地控制平面。使用多个工作负载，我们显示Larry将尾部延迟提高了2.3倍，达到相同的网络成本。

4.Semi-Oblivious Traffic Engineering:The Road Not Taken
我们期望网络能够在广泛的运行条件下提供可靠的性能，但现有的流量工程（TE）解决方案针对性能或稳健性进行了优化，但不是两者兼而有之。影响TE系统质量的关键因素是用于传输流量的一组路径。一些系统依赖最短路径，这会导致带有瓶颈链路的拓扑过度拥塞，而另一些系统使用最小化拥塞的路径，这些路径很脆弱并且容易失败。本文提出了一个系统，该系统使用一组使用Räcke的显式路由算法计算的路径，以及一个集中控制器来动态调整发送速率。尽管显式路由和集中式TE已经被孤立地研究过，但它们的组合是新颖且强大的。我们建立了一个软件框架来模拟TE解决方案，并在大量拓扑和场景中进行了广泛的实验，其中包括大型内容提供商和ISP的生产骨干。我们的研究结果表明，半显式路由提供接近最优的性能，并且比最先进的系统更加稳健。

5.Stateless Datacenter Load-balancing with Beamer
数据中心负载平衡器（或多路复用器）将去往特定服务的流量定向到一组动态后端机器。为确保后端到来或离开时的一致性负载平衡决策，现有解决方案针对每个连接做出负载平衡决策，然后将其存储为将来数据包的连接状态。虽然实现起来很简单，但每个连接一个状态的做法很脆弱：SYN-flood恶性攻击很容易填满连接状态内存，阻止复用器保存良性连接的状态数据。我们介绍Beamer，一款旨在确保无状态多路复用操作的数据中心负载均衡器。关键的想法是利用已经存储在后端服务器中的连接状态，以确保连接永远不会在外界搅动(即攻击)下被丢弃：当服务器收到没有状态的中间连接数据包时，会将其转发给另一台应该有这个数据包的状态的服务器。无状态负载平衡带来许多好处：我们的Beamer的软件实现速度比Google的Maglev快两倍，它是最先进的软件负载平衡器，可以在7个核上处理40Gbps的HTTP上行链路流量。如我们的P4实现所示，Beamer易于在软件和硬件中进行部署。最后，Beamer允许在不丢失任何连接的情况下实现任意横向伸缩事件。

6.Approximating Fair Queueing on Reconfigurable Switches
当今的拥塞控制主要是通过端到端的机制实现的，网络支持很少。因此，终端主机必须合作实现最佳的吞吐量和公平性，导致效率低和性能隔离差。尽管像公平队列这样的路由器机制可以保证所有参与者的公平带宽分配，并且在某些方面证明是最优的，但他们需要在每个数据包的基础上进行复杂的流分类，缓冲区分配和调度。这些因素使得它们在高速交换机中实施起来很代价很高。在本文中，我们使用新兴的可重构交换机来开发以线速运行的公平队列的近似形式。我们利用可配置的每个数据包处理功能以及在交换机内部保持可变状态的能力，以实现跨越所有流的公平带宽分配。此外，我们设计了一个新的出队调度程序，称为旋转严格优先级调度程序，它允许我们近似有序地从多个队列传输数据包。我们在大型叶脊拓扑上的硬件仿真和软件仿真表明，我们的方案非常接近理想的公平队列，相对于TCP和DCTCP，将短流量的平均流完成时间提高2-4倍，第99个尾延迟提高4-8倍。

7.G-NET:Effective GPU Sharing in NFV Systems
网络功能虚拟化（NFV）虚拟化软件网络功能，以提供设计，管理和部署的灵活性。尽管GPU在显着加速网络功能方面显示出其力量，但由于以下原因，它们尚未有效地集成到NFV系统中。首先，在现有GPU虚拟化方法的NFV系统中，GPU被严重利用不足。其次，GPU内存中的数据隔离不能保证。第三，在CPU-GPU架构上构建高效的网络功能需要大量的开发工作。在本文中，我们提出了一个NFV系统G-NET具有支持空间GPU共享的GPU虚拟化方案，基于服务链的GPU调度器以及保证GPU中的数据隔离的方案。我们还开发了一个在G-NET上构建高效网络功能的抽象，这大大减少了开发工作。通过我们提出的设计，与现有的GPU虚拟化解决方案相比，G-NET的整体吞吐量提高了70.8％，延迟时间缩短了44.3％。

NSDI'17

1.Enabling Wide-Spread Communications on Optical Fabric with MegaSwitch
现有的有线光纤互连面临着支持生产群中广泛通信的挑战。最初的提议受到限制，因为它们一次仅支持少量机架（例如2或4个）之间的连接，切换时间为毫秒。近来，通过快速时分共享分布在更多节点上的光纤电路来部分地缓解该问题，从而将光电路重新配置时间减少到微秒，但仍受限于可同时并行电路的总数量。在本文中，我们寻求一种光互连，可以在数千台服务器的计算集群内实现无约束的通信。 MegaSwitch是一种多光纤环形光纤结构，利用跨多个光纤的空分多路复用技术，为30多个机架和6000多台服务器提供可重新排列的非阻塞通信。我们采用商用光学设备实现了5机架40服务器MegaSwitch原型，并使用测试台实验以及大规模模拟来探索MegaSwitch的架构优势和折衷。

2.Flexplane:An Experimentation Platform for Resource Management in Datacenters
Flexplane使用户能够对数据平面算法进行编程，并进行实验，以硬件线路速率在其上运行实际应用流量。 Flexplane在软件路由器的过去工作和可编程硬件芯片组上新兴工作之间的设计空间中探索了一个中间点。与软件路由器一样，Flexplane使用户能够用高级语言（C ++）表达资源管理方案，但与软件路由器不同，Flexplane的运行速度接近硬件线路速率。为了实现这两个目标，集中式仿真器如实地在多核机器上实时模拟所需的数据平面算法，并具有非常简洁的原始数据包表示。真实数据包在仿真器通知网络时会遍历网络，与仿真对象共享相同的流程和相对延迟。 Flexplane可准确预测RED和DCTCP等几种网络方案的行为，可在10核机器上支持高达760 Gbits / s的总吞吐量（比软件路由器快20倍），并支持实际操作系统的实验和以线速运行在不同网络方案上的应用程序（例如Spark），包括当前硬件中不可用的HULL和pFabric等应用程序。

3.Flowtune:Flowlet Control for Datacenter Networks
快速收敛到期望的网络资源分配对于端点流量是一个难题。原因是拥塞控制决策分布在多个端点之间，这些端点响应于应用需求和逐个分组的基础上的网络反馈的变化而改变其提供的负载。我们针对数据中心网络提出了一种不同的方法，即流量控制，其中拥塞控制决策是在flowlet粒度而不是数据包的粒度下进行的。通过flowlet控制，分配只有在flowlet到达或离开时才会改变。我们在一个名为Flowtune的系统中使用集中式分配器实现了这个想法，该分配器接收来自端点的flowlet开始和结束通知。分配器使用一种新的快速网络效用最大化方法计算最优速率，并更新端点拥塞控制参数。实验表明，Flowtune在各种设置的尾部数据包延迟方面优于DCTCP，pFabric，sfqCoDel和XCP，在几个数据包内收敛到最佳速率，而不是在几个RTT上收敛。 EC2部署的基准测试比Linux的Cubic显示更公平的比率分配。数据汇总基准显示1.61倍较低的p95 coflow完成时间。

4.Let it Flow:Resilient Asymmetric Load Balancing with Flowlet Switching
数据中心网络需要高效的多路径负载平衡来实现高二等分带宽。尽管近年来在应对这一挑战方面取得了很大进展，既简单易行又能适应网络不对称的负载均衡设计仍然难以实现。在本文中，我们展示了flowlet交换机，这是十多年前首次提出的一个想法，它是一种用于弹性负载均衡和非对称性的强大技术。 Flowlets具有显着的弹性特性：其尺寸根据路径上的交通状况自动变化。我们使用这种见解来开发LetFlow，这是一种非常简单的负载平衡方案，可以抵御不对称。 LetFlow只需随机选择路径，让它们的弹性自然平衡不同路径上的流量。我们对真实硬件和数据包级仿真的广泛评估表明，LetFlow非常有效。尽管简单得多，但它在不对称场景下比WCMP和Presto等其他疏忽流量方案显着更好，同时在测试实验中实现CONGA的10-20％范围内的平均流完成时间和在具有大不对称性的模拟拓扑和交通负载很重情况下比CONGA改善两倍。

5.RAIL:A Case for Redundant Arrays of Inexpensive Links in Data Center Networks
虽然有很多提议可以降低数据中心网络（DCN）的成本，但很少关注携带数据包的物理链路所起的作用。通过研究跨越多个实际生产环境DCN的超过300K条光链路，我们证明这些链路相对于IEEE标准中的要求非常保守地运行。受此观察的启发，为了降低DCN成本，我们建议以超出目前规定的限制来使用收发器 - DCN成本的关键因素。我们对多种商品收发器的实验表明，它们可以“拉伸”1.6到4倍于其规格。但是，通过拉伸，1-5％的DCN路径的性能可能低于IEEE标准。我们开发RAIL系统，以确保在这样的网络中，应用程序只使用满足其性能需求的路径。我们的建议可以将10Gbps网络的网络成本降低10％，对于40Gbps网络降低44％的网络成本，而不影响应用的性能。

NSDI'16

1.Enabling ECN in Multi-Service Multi-Queue Data Centers
最近的提议利用显式拥塞通知（ECN）来实现高吞吐量低延迟数据中心网络（DCN）传输。 然而，其中大多数隐含地假设每个交换机端口具有一个队列，使得他们设计的ECN方案不适用于生产DCN，其中每个端口的多个服务队列被用来通过加权公平共享隔离不同的业务类别。 在本文中，我们通过利用广泛的测试平台实验来揭示此问题，以探索多队列场景中吞吐量，延迟和加权公平共享之间的内在折衷。 使用从勘探中学到的指导原则，我们设计了MQ-ECN，这是一种简单而有效的解决方案，用于为多业务多队列生产DCN启用ECN。 通过一系列测试实验和大规模模拟，我们展示了MQ-ECN通过同时提供高吞吐量和低延迟来打破折衷，同时仍然保持加权公平共享。

2.HUG:Multi-Resource Fairness for Correlated and Elastic Demands
在本文中，我们研究如何在多资源环境（如公共云）中最优地提供隔离保证，在这种环境中，租户对不同资源（链接）的需求是相关的。与以前的工作不同，如主导资源公平（DRF）假定静态和固定需求，我们考虑弹性需求。我们的方法将规范最大最小公平推广到具有相关需求的多资源设置，并将DRF扩展到弹性需求。我们考虑两个自然优化目标：从租户的角度来看的隔离保证和从运营商的角度来看的系统利用率（连续工作）。我们证明，在公共云网络等非合作环境中，当需求具有弹性时，在最佳隔离保证和工作保护之间存在很强的折衷。更糟的是，连续工作甚至可以降低网络利用率，而不是在需求缺乏弹性时改善网络利用率。我们找出折衷背后的根本原因，并提出一个可证明的最优分配算法，即高利用率保证（HUG），以在不牺牲DRF的最佳隔离保证，策略验证和其他有用属性的情况下实现最大可用网络利用率。在私有数据中心网络等合作环境中，HUG实现了最佳的隔离保证和连续工作。分析，模拟和实验表明HUG提供了更好的隔离保证，更高的系统利用率和更好的租户级性能。

3.Maglev:A Fast and Reliable Software Network Load Balancer
Maglev是Google的网络负载平衡器。它是一个运行在商用Linux服务器上的大型分布式软件系统。与传统的硬件网络负载平衡器不同，它不需要专门的物理机架部署，并且可以通过添加或删除服务器轻松调整其容量。网络路由器通过等价多路径（ECMP）将数据包均匀地分发给Maglev机器;每台Maglev机器然后将数据包与其相应的服务进行匹配，并将它们均匀分散到服务端点。为了适应高速且不断增长的流量，Maglev专门针对数据包处理性能进行了优化。单个Maglev机器可以使小数据包饱和10Gbps链路。 Maglev还配备了一致的哈希和连接跟踪功能，以最大限度地减少意外故障和连接导向协议故障带来的负面影响。自2008年以来，Maglev一直服务于Google的流量。它持续全球的Google服务快速增长，并且还为Google云平台提供网络负载平衡

4.Universal Packet Scheduling
在本文中，我们解决一个看似简单的问题：是否有通用的分组调度算法？更准确地说，我们分析（在理论上和经验上）是否存在单个分组调度算法，在网络级别上，它可以完美地匹配任何给定调度算法的结果。我们发现通常答案是“否”。然而，我们在理论上表明，经典的Least Slack Time First（LSTF）调度算法最接近于通用性，并且凭经验证明LSTF能够密切重放各种调度算法。然后，我们通过查看流行的性能指标（例如平均FCT，尾部数据包延迟和公平性）评估LSTF是否可以用于实践以满足各种网络目标;我们发现LSTF的表现与他们每个人的最新技术水平相当。我们还讨论了如何在不改变网络核心的情况下将LSTF与主动队列管理方案（如CoDel和ECN）结合使用。

5.FairRide:Near-Optimal, Fair Cache Sharing
内存缓存仍然是许多系统的关键组件。近年来，大量的数据存入主存，特别是在共享环境中，例如云。这种环境的性质需要资源分配来为多个用户/应用程序提供性能隔离，并为系统提供高利用率。我们研究了共享文件的多个用户公平分配内存缓存的问题。令人惊讶的是，我们发现没有内存分配策略可以提供通常可以通过其他类型的资源（例如CPU或网络）实现的所有三个理想属性（隔离保证，策略验证和帕累托效率）。我们还表明，存在实现这三个属性中的任何两个的策略。我们发现实现隔离保证和防范策略的唯一途径就是通过阻塞，我们可以在一个名为FairRide的新策略中有效地适应。我们在一个流行的以内存为中心的存储系统中实现了FairRide，使用了一种有效的阻塞形式，并命名为预期的延迟，并证明FairRide可以在许多场景中实现更高的缓存效率（2.6倍于独立缓存）和公平性。

6.Ernest:Efficient Performance Prediction for Large-Scale Advanced Analytics
最近的工作量趋势表明，在云计算基础架构上部署机器学习，基因组学和科学计算方面发展迅速。但是，在共享基础架构上有效运行这些应用程序是具有挑战性的，我们发现选择正确的硬件配置可以显着改善性能和成本。应对上述挑战的关键是能够预测各种资源配置下应用的性能，以便我们能够自动选择最佳配置。我们的见解是，一些工作在计算和通信方面具有可预测的结构。因此，我们可以基于在小数据样本上任务的行为来构建性能模型，然后在更大的数据集和集群大小上预测其性能。为了最大限度地减少构建模型所花费的时间和资源，我们使用最优实验设计，这是一种统计技术，允许我们根据需要收集尽可能少的训练点。我们已经构建了大型分析的性能预测框架Ernest，并且我们使用多个工作负载评估了Amazon EC2，结果表明我们的预测误差较低，而长期工作的训练开销低于5％。




SIGCOMM'17

1.Re-architecting datacenter networks and stacks for low latency and high performance
现代数据中心网络通过冗余Clos拓扑结构和低交换延迟提供了非常高的容量，但传输协议很少提供匹配的性能。我们提出了NDP，这是一种新颖的数据中心传输架构，能够在广泛的场景（包括incast）中实现短传输和高流量吞吐量的接近最佳完成时间。 NDP交换机缓冲区非常浅，当它们填满交换机时，将数据包报头切下并优先转发报头。这为接收者提供了来自所有发送者的瞬时需求的完整视图，并且是我们创新的高性能多路径感知传输协议的基础，可以优雅地处理大量的incast事件，并优先处理来自RTT时间标度的不同发送者的流量。我们在带有DPDK的Linux主机，软件交换机，基于NetFPGA的硬件交换机和P4中实施了NDP。我们在我们的实施和大规模仿真中评估NDP的性能，同时展示对非常低延迟和高吞吐量的支持。

2.DRILL:Micro Load Balancing for Low-latency Data Center Networks
简单数据中心网络结构的趋势将大多数网络功能（包括负载平衡）从网络核心中分离出来，并将其推向边缘。这减缓了对数据中心丢包的主要罪魁 - 微突发流的反应。我们调查相反的方向：稍微聪明的结构可以显着改善负载平衡吗？本白皮书介绍了DRILL，这是Clos网络的数据中心架构，可执行微型负载平衡，以尽可能均匀地在微秒时间范围内分配负载。 DRILL根据本地队列占用率和随机算法在每个交换机上采用每个数据包的决策来分配负载。我们的设计解决了分组重新排序和拓扑不对称所带来的关键挑战。在使用详细交换机硬件模型和实际工作负载进行仿真时，DRILL优于最近基于边缘的负载平衡器，特别是在重负载情况下。例如，在80％的负载下，其平均流量完成时间比最近的提议做法低1.3-1.4倍，主要是由于上游队列较短。为了测试硬件的可行性，我们在Verilog中实施了DRILL，并估计其开销小于1％。最后，我们分析了DRILL的稳定性和吞吐量效率

3.Credit-Scheduled Delay-Bounded Congestion Control for Datacenters
数据中心中的小型RTT（〜几十微秒），突发流量到达以及大量并发流量（数千个）给拥塞控制带来了根本性挑战，因为它们要么迫使流量每RTT至多发送一个数据包，要么诱发大量队列建立。浅缓冲交换机的广泛使用也使得主机在突发中产生许多流时更具挑战性。另外，随着链路速度的增加，逐渐探测带宽的算法需要很长时间才能达到公平份额。理想的数据中心拥塞控制必须提供1）零数据丢失，2）快速收敛，3）缓冲区占用率低，以及4）高利用率。但是，这些要求提出了相互冲突的目标。本文提出了一种新的激进方法，称为ExpressPass，它是一种面向数据中心的端到端信贷调度，时延有界拥塞控制。即使在发送数据包之前，ExpressPass也使用信用数据包来控制拥塞，这使我们能够实现有界延迟和快速收敛。它优雅地处理突发流量到达。我们使用商用交换机实施ExpressPass，并使用测试台实验和模拟提供评估。 ExpressPass在10 Gbps链路中的收敛速度比DCTCP快80倍，并且随着链路速度变得更快，差距也随之增加。它极大地提高了繁重工作负载下的性能，并显着减少了流程完成时间，特别是对于实际工作负载下的RCP，DCTCP，HULL和DX，与中小型流量相比

4.Resilient Datacenter Load Balancing in the Wild
生产数据中心在各种不确定因素下运行，如流量动态，拓扑不对称和故障。因此，数据中心负载平衡方案必须对这些不确定性具有弹性;即，他们应该准确地感知路径状况并及时作出反应以减轻后果。尽管付出了巨大的努力，但现有解决方案具有重要缺点一方面，像Presto和DRB这样的解决方案对路径条件没有意义，并且以固定粒度盲目重新路由。另一方面，像CONGA和CLOVE这样的解决方案可以感知拥塞，但它们只能在网络流出现时重新路由;因此，他们不能总是及时地对不确定性作出反应。更糟糕的是，这些解决方案无法检测/处理诸如黑洞和随机数据包丢失之类的故障，这大大降低了它们的性能。在本文中，我们将介绍Hermes，这是一款能够抵御上述不确定性的数据中心负载平衡器。Hermes的核心是利用全面的感应技术来检测路径状况，包括之前无人看管的故障，并采用及时而谨慎的重新路由做出反应。 Hermes是一款实用的基于边缘的解决方案，无需更换交换机。我们用商用交换机实现了Hermes，并通过试验台实验和大规模模拟进行评估。我们的研究结果表明，Hermes在正常情况下达到了与CONGA和Presto相当的性能，并且处理不确定性：在不对称情况下，Hermes比CONGA和CLOVE的流动完成时间（FCT）高10％和20％;在交换机故障的情况下，其性能超过所有其他方案32％以上。

5.Understanding and Mitigating Packet Corruption in Data Center Networks
我们全面了解数据中心网络中的数据包损坏情况，从而导致数据包丢失和应用程序性能下降。通过研究15个生产数据中心的350K链接，我们发现腐败损失程度显着，其特征与拥塞丢失明显不同。损坏对链路的影响比拥塞的影响小，但是造成了较高的损失率;与拥堵不同，链路上的损坏率随时间稳定并与其利用率无关。基于这些观察，我们开发了CorrOpt，一种减轻损坏的系统。为了最大限度地减少损坏损失，它会智能地选择可以安全禁用哪些损坏链路，同时确保每个架顶式交换机都有最少数量的路径到达其他交换机。根据我们对损坏根源不同的常见症状的分析，CorrOpt还建议采取具体行动（例如更换电缆，清洁连接器）以修复禁用的链接。我们的推荐引擎已经部署在大型云提供商的七十多个数据中心中。我们的分析表明，与现有技术相比，CorrOpt可以将腐败损失减少三到六个数量级，并将修复精度提高60％。


SIGCOMM'16
1.2DFQ:Two-Dimensional Fair Queueing for Multi-Tenant Cloud Services
在许多重要的云服务中，不同租户在相同流程的线程池中执行请求，需要公平地共享资源。 然而，使用公平队列调度器来提供公平性在这种情况下是困难的，因为执行并发性高，并且因为请求成本是未知的并且变化大。 在这种设置中使用WFQ和WF2Q等公平的调度程序会导致突发的调度，其中大量请求会阻塞小的请求很长时间。 在本文中，我们提出了二维公平排队（2DFQ），它将不同成本的请求分散到不同的线程中，并最大限度地减少了租户对不可预知请求的影响。 在评估来自Azure存储的生产工作负载时，我们展示了2DFQ将服务的突发性降低了1-2个数量级。 在许多大型请求与小型请求竞争的工作负载上，2DFQ将99个百分点的延迟改善了2个数量级。

2.CODA:Toward Automatically Identifying and Scheduling COflows in the DArk
最近，利用使用colfows的应用程序级别的要求显示出可以提高数据并行群集中的应用程序级通信性能。然而，现有的基于coflow的解决方案依赖于修改应用程序来提取coflow，使得它们不适用于许多实际场景。在本文中，我们介绍了CODA，这是第一次尝试自动识别和调度coflow，而无需修改任何应用程序。我们采用增量聚类算法来执行快速的，应用程序透明的coflow识别，并通过提出容错coflow调度程序来缓解偶然的识别错误来补充它。测试平台实验和大规模的生产工作负载模拟表明，CODA以超过90％的精确率正确识别coflows，并且其调度程序对不准确性具有很强的鲁棒性，使通信阶段平均可以比每条流机制快2.4⇥（5.1⇥）(95百分位)倍。总体而言，CODA的性能与需要修改应用程序的解决方案相当。

3.Scheduling Mix-flows in Commodity Datacenters with Karuna
云应用程序会生成带有和不带有最终期限的混合的网络流。调度这种混合流是一个关键的挑战。我们的实验表明，将截止期限/非截止期限流量的现有方案简单地结合是有问题的。例如，优先处理截止日期流量会损害非截止日期流量的流量完成时间（FCT），而对截止日期错过率的影响略有改善。我们提供了Karuna，这是第一个用于调度混合流的系统解决方案。我们的主要见解是，截止日期流量应该在截止日期前完成，而对非截止日期流量的FCT影响最小。为了实现这一目标，我们设计了一种新颖的最小影响拥塞控制协议（MCP），它以尽可能少的带宽来处理截止期流量。对于非截止期流量，我们扩展现有的FCT最小化方案以调度已知和未知大小的流量。 Karuna不需要交换机修改，并且与传统的TCP / IP堆栈向后兼容。我们的试验台实验和模拟表明，Karuna有效地调度混合流量，例如，与pFabric相比，在高负荷时将非终点流量的第95百分位FCT降低高达47.78％，同时保持低限（<5.8％）的最终失败率

4.NUMFabric:Fast and Flexible Bandwidth Allocation in Datacenters
我们提供NUMFabric，一种新颖的传输设计，提供灵活和快速的带宽分配控制。 NUMFabric非常灵活：它可以让运营商指定如何在竞争流之间分配带宽，以针对不同的服务级别目标进行优化，例如加权公平，最小化流程完成时间，多路径资源池，优先级带宽功能等等。NUMFabric也非常快速： 它比以前的方案收敛到指定分配快2.3倍。 底层NUMFabric是一种新颖的分布式算法，利用网络中加权公平排队数据包调度快速收敛来解决网络效用最大化问题。 我们使用真实的数据中心拓扑和高度动态的工作负载评估NUMFabric，并表明它能够在这种压力环境中提供灵活性和快速收敛。































