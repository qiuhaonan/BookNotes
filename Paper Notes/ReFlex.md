# ReFlex : Remote Flash ≈ Local Flash

## 1. 背景和动机

大背景：NVMe 闪存设备每秒可执行1百万次I/O操作，而时延在100us以内，这种特性使得他们成为许多数据密集型在线服务的存储介质。

大问题：由于不同应用和随着时间变化时应用对闪存设备的需求不均衡，部署在数据中心的闪存设备的容量和吞吐量通常没有被充分利用。

直观解法：设计机器时为所有的负载来平衡CPU，内存和闪存资源。

直观解法的问题：很难设计的完美，总会造成资源过剩和更高的运营成本。

优化解法：通过网络来远程共享和访问闪存资源可以很好的改善利用率，提高使用闪存设备的灵活性，并且降低了运营成本。

## 2. 问题和挑战

**问题1：低时延**

在共享数据中心磁盘时，磁盘I/O本身的高时延和低吞吐量掩盖了网络开销。但是在共享闪存设备时，闪存设备的时延本身就很低(未加载读取时延为20-100us)，而仅仅通过传统的10GbE网络和TCP/IP协议栈就需要至少50us时延，还不算基于软件的存储协议所带来的开销。更重要的是，由于linux的中断管理和核调度给网络访问引入了不可预测的性能，并且增加了尾延迟。

网络存储系统(iSCSI)引入协议处理和在用户空间与内核空间之间数据拷贝需要的额外时延。另一方面，向GFS和HDFS这种分布式文件系统是为向远程磁盘传输MB级数据优化的，而对于向远程闪存访问KB级数据会引入额外开销。

**问题2：以低代价实现高吞吐量**

现代闪存设备的吞吐量在百万IOPS级别，远程闪存服务器必须以低处理开销来服务高I/O请求率才能降低整体代价。而现在基于软件的解决方法需要非常多的计算资源才能打满闪存的吞吐量。例如: 对于iSCSI协议，每个核可以达到70K IOPS，那么要达到1百万IOPS就需要14个核。而数据中心的机器可能没有这么多的计算资源来服务远端的请求。

**问题3：干扰管理**

读操作的尾延迟取决于负载(吞吐量)和读写比例，因为写操作更慢并且触发磨损均衡和垃圾回收机制，这些机制的开销不总是能被覆盖掉。对于使用本地闪存设备的单个应用来说，读写干扰可以被管理起来，但是对于多个共享同一个闪存设备且互不感知的租户，读写管理就成了一个大问题。

硬件加速并不够：干扰管理是NVMe over Fabrics, QuickSAN, iSER等硬件加速解决方案的一个主要漏洞。目前NVMe设备的隔离特性：硬件队列，命名空间，没有额外的软件辅助的话，并不足以降低多个远端客户之间的干扰。因为队列数量有限(高端设备有64个)并且请求仲裁简单(RR)。命名空间是主机端对设备的逻辑分区，因此对于不同命名空间发起的请求仍然会相互干扰。目前的NVMe over Fabrics解决方案并不执行I/O调度，也不给客户端提供可以指定QoS的方式，因此就不能以QoS-aware的方式来管理闪存设备。

## 3.ReFlex设计

### 3.1 数据面板执行模型

每个ReFlex服务线程使用绑定的核，可以直接并且独占的访问网络队列来收发数据和NVMe队列来提交和完成闪存命令。

首先，网卡从网络上受到数据并通过DMA把数据放到协议栈预先分配的缓存中；然后ReFlex服务线程poll接收环并通过以太网驱动和网络协议栈来处理数据包，并产生条件事件来表示新消息的到来；接着，该线程通过libix来处理事件，切换到服务代码来解析消息，提取I/O请求并执行访问控制检查和其他存储协议处理；然后，该线程执行I/O调度并提交I/O请求到闪存设备的硬件队列中；接着，闪存设备通过DMA把数据读取到预先分配的用户空间缓存中；然后，该线程poll完成队列并产生完成事件，调用响应的回调函数来发起一个send系统调用；最终，该线程处理send系统调用来把请求的数据通过网络栈发回给远端客户端。整个执行模型划分为两个部分：网络数据收取到闪存命令提交，闪存命令完成到网络数据发送。

**延迟**:整个过程是由一个线程运行直到结束，检测收发数据和检测完成都是通过poll操作，中间不涉及中断或者线程调度，消除了延迟波动并且改善了请求处理中数据缓存局部性。在收发数据时，通过传递内存指针来实现零拷贝。

**吞吐量**: 在第一个阶段末尾使用异步I/O提交方式来覆盖闪存读写延迟。线程可以去处理剩余的第一个阶段任务而不必卡在第二个阶段的开头。其次，通过自适应batch处理来应对高负载时的任务，可以平衡吞吐量与延迟。

使用多个服务线程时，这些线程只需要在向硬件队列发起NVMe命令时做一次协调来保证尾延迟和吞吐量的SLO(Service Level Objectives)。

### 3.2 QoS调度和隔离

一个SLO指定了在特定吞吐量和读写比例上的读取尾延迟限制。

除了这种延迟关键的租户，ReFlex还服务best-effort租户，它们会机会地使用任何未分配或未使用的闪存带宽，并且可以容忍更高的时延。一个租户的定义可以被上千的网络连接共享，它们来自不同的客户机上运行的任意数量的应用。一个应用可以使用多个不同的租户来对不同的数据流请求不同的SLOs。

在闪存设备访问上执行SLO因两个因素而变得复杂：

- 设备可以支持的最大带宽(IOPS)取决于所有租户上看到的整体读写比例。

- 读请求的尾延迟取决于整体的读写比例和当前的带宽负载。

  

因此，QoS调度器需要全局可视性和控制闪存设备上的整体负载以及未完成I/O操作的类型。我们使用一个请求代价模型来计算每个闪存I/O请求对读操作尾延迟的影响，和一个新的调度算法在所有的租户和数据面板线程上来保证SLOs。ReFlex不假定对负载有先验知识。

#### 3.2.1 请求代价模型

该模型以加权IOPS函数来估计读操作的尾延迟，请求的代价cost取决于I/O size,类型(read/write)，和当前设备上的读写请求比例r

$I/O\ cost = \lceil\frac{I/O\ size}{4KB}\rceil \times C(I/O\ type, r)$

cost是r的一个函数，因为在不同的读写比例下，设备支持的IOPS不同，因此，当设备上的负载是只读操作时，模型会调整读请求的代价。Cost以多个token来表达，每个token代表一个4KB随机读的代价，在我们使用的闪存设备上，代价随着请求的大小线性增加(>4KB)。对于写操作，使用随机写模式来触发最坏的情况。然后，使用曲线拟合来提取函数C(I/O type, r)。在部署之后，为了考虑到由于闪存磨损均衡带来的性能下降，这个模型可以被重新计算。

#### 3.2.2 调度机制

**Token管理**:调度器以闪存设备在给定尾延迟SLO下能够提供的最大加权IOPS的速率来产生token。ReFlex以所有租户中最严格的尾延迟SLO作为计算SLO。然后根据用户指定的吞吐量和读写比例，来计算每秒分配给该租户的token数量。调度器产生的tokens没有分配给延迟关键的租户时，将会公平的分配给Best-effort租户。调度器在向闪存设备提交请求时，根据每个请求的代价来花费tokens。

**Scheduling算法**:每个服务线程把闪存请求入队到每个租户的软件队列中。当该线程执行到调度步骤时，它会计算每个入队请求的加权代价，然后提交所有可行的请求到闪存设备中。取决于线程的负载和批处理因子，执行模型每0.5us到100us进入一次调度例程。控制面板和批处理极限确保两次调度之间的时间不会超过最严格的SLO的5%。频繁的调度对于避免过多的排队延迟和保持NVMe设备高利用率很重要。

**Latency-critical租户**:调度算法最先服务LC租户，首先调度器根据它们的SLO和距离上次调度经过的时间来为每个LC来生成tokens。由于流量几乎不会是均衡的，并且租户也许会发起比SLO中预留的IOPS多或少，调度器必须追踪每个租户的token使用。允许租户短暂地超出分配的token来避免短期排队，但是一旦它们到达token赤字的极限时，会通过限制LC租户速率来限制burst size。那些消耗少于可用token的LC租户能够累计这些token，来为以后的burst服务，但是累计有上限，当触及上限时，调度器会把一大部分的token转移给Best-effort租户使用的全局token桶中。上限值被设置被最近三次分配给LC租户的token数量，用以容纳短期burst而不会造成token赤字。

**Best-effort租户**:调度器从闪存设备剩余未分配的吞吐量中，为每个BE租户公平的分配tokens。闪存设备剩余未分配的吞吐量是指在能够保证最严格的LC延迟SLO下，设备能够提供的token速率减去LC租户token速率的值。当BE租户的token不够时，可以从全局的token桶中申请token。服务线程以RR方式来服务BE租户。当一个BE租户的软件队列为空时，禁止积累tokens。BE租户没有使用的token也会被转移到全局token桶中以供其他BE租户使用。为了防止全局token桶过大，周期性的重置全局桶。
