强调 底层细节比如 独立的PCIe事务和NIC结构
在RDMA能力和应用程序之间找到高效匹配


PCI Express
PCIe是一个层次化协议，每一层的头部会增加中间耗费，这对理解效率很重要。
RDMA操作会产生3种类型的PCIe事务层报文：读请求，写请求和读完成，没有对一次写的事务层反应。

MMIO writes VS DMA reads：
两种从CPU到PCIe设备传输数据的方法
CPU写到映射设备内存来初始化PCIe写。为了避免每个存储指令都产生一个PCIe写，CPU使用了一个叫做 写结合 的优化方法，它把存储的数据
合并来生成cache行大小的PCIe事务。
带有DMA引擎的PCIe设备可以使用DMA从DRAM中读取数据。DMA读不限制于缓存行大小，但是比CPU的读完成合并大小(Crc)大得多的一个读的response被
切分成多个完成。Crc是128字节在我们使用的intel cpu设备上。一次DMA读总是比相同大小的MMIO使用更少的host-to-device PCIe带宽。


PCIe计数器：
  我们的贡献主要依赖于理解在NIC和CPU之间的PCIe交互。我们的分析主要使用用于DMA读的计数和DMA写的计数。
  
RDMAverbs和传输类型
  一旦完成一个verb，请求者(发起一个verb的一方)的NIC选择性地通过把一个CQE DMA到和这个QP关联的CQ中来发出完成信号。
  这两种类型的verbs是内存verbs和消息verbs。
  内存verbs包括RDMA读，写和原子操作。
  消息verbs包括send和recv。这两个操作会牵涉到responder的CPU：send的负载会被responder的cpu post的recv里指定的地址上。
  
RDMA WQEs：
  为了初始化RDMA操作，在requster一边的用户模式NIC网卡在host memory创建WQE，通常，WQE是在预分配的连续内存区域里创建的，每个WQE
  是独立缓存对齐的。
  首先，假设NIC是基于PCIe的设备，就是说NIC并不集成在芯片上。其次，假设NIC内部使用多个处理单元来完成并行化。
  为了讨论使用下面的优化对CPU和PCIe的影响，我们认为从CPU到NIC传输N个D字节大小的WQEs
  减少CPU发起的MMIO:
    如果减少MMIO的数量或者替换成节省CPU和带宽的DMA，CPU效率和RDMA吞吐量都会得到改善。
    CPU通过MMIO给NIC发送消息来初始化网卡操作。
    这个消息可以包含：1.新的WQE 或者 2.对新的WQE的引用
    第一种情况下，WQE通过64字节 写合并的 MMIOs来传输。(WQE-by-MMIOs)(BlueFrame)(PIO)
    第二种情况下，NIC通过一次或多次DMA来读WQE。(Doorbell)(Doorbell)(SDMA)
    WQE-by-MMIOs方法是为低延迟做优化，通常是默认的方法。
    两种优化可以通过减少MMIOs来改善性能：
      1.Doorbell batching：应用一次发出多个WQEs到一个QP，它可以使用一次Doorbell MMIO来做一次batch
      2.WQE shrinking：减少被一个WQE使用的缓存行的数量可以大大地改善吞吐量。
      
  减少NIC发起的DMAs：
    减少DMA数量可以节省NIC的处理能力和PCIe的带宽，改善RDMA的吞吐量。
    注意到上面的batching优化增加了一次DMA读，但是避免了多次MMIOs，这通常是一个好的折中。
    已知的用来减少NIC发起的DMAs包括unsignaled verbs来减少完成的DMA写（CQE）和负载内联来避免负载DMA读（inline data）。
    batching和shrinking也可以用来优化DMA次数。
    NIC必须对每一个完成的RECV DMA一个CQE。和其他种类那种只提醒完成并且是不重要的的verbs不一样，RECV的CQEs包含重要的元数据，
    比如收到数据的大小。NIC通常会为负载和完成产生两个分离的DMA，分别把他们写到应用程序享有的内存和驱动享有的内存里。
    我们假设对应一个RECV的SEND携带X字节的负载。
    Inline RECV：
      如果X很小，NIC把负载封装进CQE里，这个CQE后来会被驱动复制到应用程序指定的地址空间里去。
    Header-only RECV：
      如果X=0，就是没有负载的情况下，在接收方就不会产生负载的DMA。报文的头部里的一些信息被包含在DMA发出的CQE中，可以用来实现应用协议。
      那么就只会产生一次DMA而不是两次DMA
    
    加入多个NIC处理单元：
    通常RDMA编程决策都是尽量使用更少的QP对，但是这样做就把NIC的并行化限制在QP数量上了。
    这是因为在相同QP上的操作有顺序依赖并且最好被同一个NIC处理来避免跨PU同步。
    吧一个cpu核绑定到一个PU上会把CPU的吞吐量限制在PU的吞吐量上，在这种情况下，每个核使用多个QP会增加CPU效率，这叫做多队列优化。
    
    避免在NIC PUs之间竞争：
    所有可用的NIC在写的时候会为原子操作使用内部并发控制，PUs 获取对目标地址的内部锁，并且通过PCIe发起 read-modify-write.操作。
    注意到原子操作也会和非原子操作竞争。
    
    避免NIC 缓存miss：
    一次miss会变成通过PCIe读。缓存的信息包括：1.RDMA注册内存的虚拟地址到物理地址的转换 2.QP状态 3.一个WQE的缓存（有待证明）
    方法是使用大页和使用更少的QP。
    检测缓存miss：利用PCIe计数器通过检测和测量WQE的缓存miss来实现。一般化，从PCIe计数器报告的实际读的次数减去应用期望读的次数
    就能得到缓存miss的次数。
    WQE缓存miss：如果一个更新一些的WQE被赶出去，那么就可以检测出miss
    
    
    在UD QP中，CPU核和QP太少（比如1：1），瓶颈会出现在NIC 处理单元handling QP上(就是说我有很强的处理能力，但是QP太少，我没法横向增强处理量)
    CPU核的数量增加到一定程度时（1：3QP），瓶颈会出现在DMA 带宽上，就是说DMA往QP里面填数据给NIC处理的速度已经达到了极限。
    CPU核数量再增加，吞吐量就会下降，因为数据过于分散，导致response batch size很小。(占用时间)
    做batching，瓶颈在DMA带宽上
    不做batching，瓶颈在MMIO带宽上
    
    UD SEND WQE在mellanox的NIC上会因为头部的68字节而延伸2个cache行
    RECV慢是因为CQE的DMA，否则和WRITE一样快
    线性计数器需要用到fetch-and-add操作
    batching只限于数据报传输
    
    -------------------------------------------------------------
    为了避免每条存储指令CPU都产生一次PCIE写，CPU使用write-combining来产生cache-line-size的数据。
    DMA的读不严格限制数据规模。
    链路层做了拥塞控制和重传机制，做到几乎没有丢包的现象。
    CPU和外设之间的数据传输有两种：
      一种是MMIO，另一种是DMA
      MMIO是CPU发起的操作，可以传实际数据，也可以传数据的地址，然后由外设通过DMA来获取数据。
      减少DMA的次数可以使用unsignaled verbs但RECV必须有CQE，使用payload inline
      DMA可以有batch，doorbel。
