摘要：
    FaSST是一个基于RDMA支持有序性和持久性的分布式内存事务系统。
    现存的基于RDMA事务处理系统使用one-sided RDMA通信原语，因为它们能绕过远端CPU。
    这种设计选择带来一些缺点：1.设计分布式数据存储时，one-sided RDMA缺乏灵活性会降低性能和增加软件复杂度。
                            2.RDMA硬件的深层技术短板限制了在大规模集群的可扩展性。
简介：
    可序列化的分布式事务为像对象存储和在线事务处理系统的分布式系统设计提供强有力的编程抽象。
    虽然以前在这方面的工作为性能奉献了健壮的事务语义，最近的一些系统已经展现了数据中心的事务可以很快。
    关键因素就是高速网络和轻量级网络栈。这些系统的公共特征就是他们广泛使用one-sided RDMA操作，可以绕过远端cpu。
    在这片文章里，我们探求是否one-sided RDMA是不是设计事务处理系统的最佳选择。
    
    首先，在one-sided RDMA范式和访问远程数据存储的高效事务所需的能力之间存在分歧。
    one-sided RDMA只提供读，写和原子操作，而访问远程数据存储通常都涉及遍历数据结构如哈希表和B树。
    通常这些结构都包含一个用于快速查询的索引和实际的数据，需要两个或两个以上的RDMA读来访问数据。
    这会导致低带宽和高延迟，从远端cpu旁通而减少了网络处理器的节省（也就是说远端cpu负载减少了，但是网络处理器的负载增加了）
    克服这个分歧的关键方法就是展平数据结构(flatten)，要么去掉索引，要么把数据和索引合并在一起，或者在所有的服务器上缓存索引。
    
    其次，当前one-sided RDMA实现的以连接为导向的本性通常需要cpu核之间共享本地NIC queue pairs 获得扩展性，而本地每个核的RDMA吞吐量
    和绕过远端cpu获得的网络收益都减少了几分之一。
    通过two-sided 不可靠数据报消息的rpc，在消息处理上牵涉到远端cpu，比one-sided RDMA更加灵活，允许数据访问在单个往返内完成。
    FaSST：一个多对多的RPC系统  允许Doorbell batching（通过减少CPU初始化的 pcie总线事务数量来节省CPU周期）
 
背景：
    快速分布式事务：
        FaSST目标是提供在单个数据中心内的分布式事务，这里系统个单个实例可以扩展到几百个结点。
        系统中的每个结点负责一块基于一个主键的数据，并且每个结点既是server又是client，对于数据局部性好的负载(只访问一块内数据的事务)，这种
        对称模型可以通过将它们访问的数据放在同一个事务里来获得更高的性能。
        使得数据在机器发生故障时持久化需要把事务记录到持久性存储里，快速恢复需要维持数据存储的多个备份。
        将持久存储（如磁盘或SSD）置于事务的关键路径上会限制性能。
        和最近的工作一样，FaSST假设事务处理结点配备有后备电池的DRAM。
     RDMA：
        有连接的传输提供one-sided的RDMA和端到端的可靠性，但是不能很好的扩展到大型集群上。
        这是因为NIC用来缓存QP状态的内存有限，使用太多QP而超过这个状态的大小会导致缓存抖动。
        无连接的传输不提供one-sided RDMA或者端到端可靠性。然而，他们允许一个QP可以和多个其他QP通信，并且比有连接的传输具备更好的可靠性，
        因为每个线程只需要一个QP。(一个线程可能给多个远端线程通信，但是只需要一个QP)
        现代高速网络包括mellanox的infiniband和intel的omniPath，在传输层以下提供可靠性。他们的链路层使用流控制来防止
        基于拥塞的丢包和重传来防止bit基于错误的丢包。Infiniband的物理层使用前向错误纠正(FEC)来修正大多数位错误，这个本身很少出现。
        例如：我们集群中使用的infiniband 线路的位错误率低于10^-15。也就是说有了这些基础，这些网络的链路层流控制在极少情况下会出现拥塞崩溃，
        带来低吞吐量，但是不会丢包。
        
选择网络原语：
    RPCs的优势：
        讨论两种降低READ数量的优化措施和他们的限制。
        索引和值在一起：
            FaRM使用一个专用索引把值存储在索引临近的位置上来提供哈希表访问在大约1次READ，允许数据和索引一起读。
            然而这样做会把READ的size放大6～8倍，减小了吞吐量。这个结果强调比较网络原语的应用级性能的重要性：虽然微测试表明READs比
            大小类似的RPCs表现要好，但是由于one-sided本性，READs需要额外的网络流量和往返次数，在另一方面限制了规模。
         缓存索引：
            在单结点事务处理系统里，流行的在线事务处理测试上索引占用超过35%的内存，和我们的分布式事务处理实现测试的比率相近。
            在我们这个情况下，缓存10%的索引需要每台机器奉献整个集群内存容量的3.5%来存储索引，但集群包含超过100/3.5=29个结点时就几乎不可能了。
            The cell B Tree 缓存B树在叶子节点之上的4层的结点来节省内存和减少抖动，但是当client使用READs来访问B树的时候就要多次往返(大概4次)。
         RPCs允许通过两个消息就能访问分块数据，请求和回复。它们不会增加消息大小，也不需要多次往返，也不用缓存。
         基于RPC编程的简单性减少了软件复杂性(在事务处理中需要利用现代快速网络：来实现一个分块的，分布式数据存储)，用户仅仅为一个单结点数据存储
         写很短的RPC处理程序。
    数据报传输的优势：
         数据报传输允许每个CPU核创建一个数据报QP可以和所有远端核通信。因为QP的数量相对较小(和核的数量一样多)，给每个核独占访问QP而不会溢出
         NIC的缓存是可以的。
         共享QP降低CPU的效率因为线程会竞争锁，cache行用于QP缓冲区的就会在CPU核之间弹跳。效果是非常巨大的：共享QP把每个核one-sided READs
         的吞吐量减少了5.4倍。
         FaRM的RPCs使用one-sided的WRITEs和共享QP在每台机器 5百万个请求每秒的时候，CPU成为了瓶颈。
         而我们的基于数据报的RPCs不要求共享QP并且每台机器达到了每台机器 40.9百万个请求每秒，即便如此，也是NIC成为了瓶颈，而不是CPU
         对比有连接传输，数据报传输的第二个优势是：DoorBell batching 降低CPU使用率
         用户进程通过在PCIe总线上写给NIC上的每个QP一个的DoorBell register，指明在这个QP上新操作的数量来把操作post给NIC
         这种写对于CPU来说代价比较高因为它需要刷新写缓冲并且使用内存障维持有序。然而，在事务系统里，应用可以通过一次发出多个RDMA工作请求(WR)
         来缓解这个代价。使用一个数据报QP，进程只需要每次批操作时ring一次Doorbell，不用管批操作里每个消息的目的地址。
         但是使用一个有连接的QP，进程就必须ring多个Doorbells，数量和批操作里目的地址的数量一样多。
         Doorbell batching 在RDMA层不会合并packets，也不会增加时延，因为我们适时的执行。(就是说唤醒一次我就做一次，不管工作量多还是少)
         
    性能考虑：
        两个最近的项目在研究RPCs和one-sided RDMA的相关性能。在非对称环境下，多个clients发送请求给单个server，HERD表明RPCs执行和READ相似。
        在HERD里，clients通过UC的WRITE给server发送请求，server通过UD的SEND发送回复。这种方法随着client的规模扩展的很好因为server上活跃
        的queue pairs数量很小，Server的UC QP是被动的，因为server的CPU并不访问它们，这些被动的QP消耗很少的NIC内存。活跃的UD QPs数量很小。
        但是在分布式事务需要的对称环境下，每台机器发出请求和回复，这种设计就并不能很好的扩展，因为每个结点上需要很多活跃的UC QPs来发送请求。
    可靠性考虑：
        第一，我们注意到事务处理系统通常包括一个重配置机制来解决结点失效。重配置包括选择性地暂停正在进行的事务，通知结点新的集群成员身份，
        回放事务日志和重复制遗失数据。我们假设一个标准的重配置机制，我们没有实现它。我们期望FaRM的恢复协议可以适用于FaSST。
        第二，在正常操作里，在现代具备RDMA的网络里，包丢失极其罕见。如果由于网卡硬件问题出现的丢包，我们可以通过RPC的requester维持的粗粒度的
        timeout来检测到
        基于以上两点观察，在FaSST中我们提出的处理丢包解决方案是：简单地重启被丢失的RPC包影响的两个FaSST进程中的一个，允许重配置机制来
        为受到影响的事务做出提交决定。
        
     RPC接口和优化：
        一个worker协程操作b>=1个请求的批量操作，b是基于程序逻辑发出的数量确定。(就是发多少做多少)
        worker先创建一个新的请求而不执行网络I/O。对于每一个请求，它会指定请求类型和目的机器的ID。
        创建一批请求后，worker调用一个RPC函数来发送请求信息。FaSST选择目的线程是远端机器上基于本地线程ID的对等线程。
        把RPC通信限制在线程对之间可以改善FaSST的可扩展性，通过减少可以给一个线程发送请求的协程的数量。
        
        批量请求：1.吧NIC的Doorbell数量从b减少到1，节省cpu周期。2.RPC层可以把发往同一个目的机器的消息合并起来。
        这对于访问带有相同主键的多个表的多键事务尤其有用（呵呵，就这个？）因为我们的事务层通过对主键进行哈希来划分表，
        表访问请求在同一个包中发送。3.批量减少了协程切换的中间耗费：主协程只有在收到所有b个请求后才让权给一个worker，把切换耗费减少了b倍。
        
        cheap RECV posting：posting RECVs 需要在宿主内存的RECV queue里创建描述符，更新队列的宿主内存头指针。
        在FaSST里，在初始化阶段我们占据这个带有描述符的RECV queue，在这之后，这些描述符不再被CPU访问；新的RECV 以循环的方式重用这些描述符，
        并且可以单次写到被缓存的头指针里来post。
        
    检测包丢失：
        每个线程的master协程检测它的worker协程发出的RPC的包丢失情况。
        master会通过计数每个worker收到的response数量来跟踪每个worker的进度。当且仅当workers中有一个worker的RPC请求或者回应丢失了，那么
        这个worker的进度计数就会停下来，master就不会收到该worker的所有response，它也不会再次调用该worker，避免又发送新的请求。
        如果收到了所有的response，那么master就会调用它，让它发送新的请求。我们不允许worker不发送请求就把处理器让给master。
        如果某个worker的计数经过timeout后没有改变,master就认为它丢包了.怀疑丢包时,master会kill掉机器上的这个进程(process)
        注意到,在检测出丢包之前,一次丢包只会影响当前这个worker的进度,其他worker可以成功提交事务直到丢包被检测到.
        这允许我们使用更大的timeout值而不会影响FaSST的可用性。
        
    RPCs的局限性：
        MTU  4KB
        FaSST限制每个协程在每次batch里对每个目的机器只能发一条message，但是这个message里可以包含多个合并的requests
        这么做是为了保持RECV队列足够小能被NIC缓存起来。
        假设N个node   每个node有T个thread   每个thread有c个coroutine  每个coroutine发送m个消息
        那么每个RECV队列就必须有N*c*m个RECV请求位置，那么m如果比较大就会导致NIC的缓存抖动而降低RPC的性能。
        如果要支持更大的集群，就必须减少RECV队列的大小。可以把从本地线程发给特定远端机器的请求数量从c减少到一个小一点的值。
        如果超出这个值，这个协程就会让出cpu。这样做比较好是因为多个协程发送请求给同一个远端机器的概率很小。
